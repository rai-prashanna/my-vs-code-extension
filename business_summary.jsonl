{"file": "app/models/metric.rb", "type": "model", "domain_rules": ["A Metric records a system event tied to an Organization—every metric must belong to an organization.", "A Metric can optionally record which User caused the event (whodunnit via user_id); when user_id is set or changed it must pass a user privacy policy validation.", "Metrics are ordered chronologically by created_at by default (implicit ordering column).", "The model uses a type field as an event/type discriminator (STI disabled), e.g. FAILED_LOGINS = 'logins.failed'.", "There is a convenience scope to retrieve failed login events (type == 'logins.failed')."]}
{"file": "app/models/label.rb", "type": "model", "domain_rules": ["A Label belongs to an Organization and can optionally be associated with a User and an Approval Team.", "Labels are typed: only equipment, product, or consumable; templates are chosen from a configured set.", "A Label must have a name, width, and height; width and height are treated as millimeter dimensions used for printing.", "Label records are versioned (history is tracked).", "Labels can be archived; archived labels are presented with an “[Archived]” prefix in their name.", "Labels require approval: a label is considered to require approval when its approved flag is false.", "Only labels that are approved and not marked as approved_clone are considered printable.", "Labels marked as editable exclude those with approved_clone = true (editable scope).", "All detail fields (detail through detail5) are sanitized before validation to remove embedded JavaScript.", "Label templates may contain a barcode/QR placeholder UUID that is replaced with an object’s id to generate per-object QR HTML; the resulting HTML is filtered prior to printing.", "Supported print/QR database fields vary by label type: equipment (Serial #, Name), product (Product ID, Name plus product custom formula fields), consumable (Model, Expiration plus consumable custom formula fields).", "When a Label is deleted, related change_notifications are not destroyed but disassociated (their foreign key is nulled).", "Label search is permission-scoped (Pundit) and matches name via case-insensitive partial match; defaults return a limited, policy-scoped set."]}
{"file": "app/models/protocols_variable.rb", "type": "model", "domain_rules": ["A ProtocolsVariable must belong to a Protocol.", "Each variable requires a name; the name must be unique within the same protocol when the record is created.", "Variables may form a hierarchy: a variable can have an optional parent and can have many children.", "If a parent variable is deleted, its children are not deleted — their parent link is cleared (parent_id set to null).", "Deleting a ProtocolsVariable destroys its associated profiles_variables.", "Displayed label is the explicit label if present, otherwise the name; if a unit is present it is appended in parentheses (e.g. \"Label (unit)\").", "Displayed hint uses an explicit hint when provided; otherwise a custom hint is generated combining min, max (if present) and a data-type requirement message.", "During import, labels and hints can be looked up from device- and protocol-specific configuration (config/eris/protocols_variables.yml) using a key format P{zero-padded-protocol_num}V{zero-padded-protocol_ver}; missing config for a device/protocol combination raises an error.", "Protocol numbers and versions are zero-padded to three digits when constructing configuration lookup keys.", "Records are implicitly ordered by created_at by default."]}
{"file": "app/models/session.rb", "type": "model", "domain_rules": ["A session must belong to a user.", "A session must belong to an organization (organization_id is stored on the session).", "Each session record is uniquely identified by a session id (id is treated as the unique/primary identifier).", "When creating a session via hit(session_id, user): if the session is new, set last_seen_at and logged_in_at to now and associate user_id and the user's organization_id.", "If creating a session fails due to a unique constraint (session already exists), do not create a duplicate — instead update the existing session's last_seen_at to now.", "Session.organization_id is derived from the provided user's organization (sessions are tied to the user's organization).", "Sessions are ordered chronologically by created_at.", "Unexpected errors during hit are logged as fatal (creation/update failures other than uniqueness are not silently ignored)."]}
{"file": "app/models/equipment_gateway.rb", "type": "model", "domain_rules": ["EquipmentGateway represents an association between an Equipment and a Gateway and must reference both entities.", "An equipment cannot be linked to the same gateway more than once — the equipment/gateway pair must be unique.", "Duplicate association records for the same equipment and gateway are prevented."]}
{"file": "app/models/software_version_base.rb", "type": "model", "domain_rules": ["This class represents a client for a remote software-version management API and must authenticate using configured credentials (email and password).", "The API endpoint (site) is determined at runtime from application configuration and can change dynamically (to support testing and environment differences).", "In production the client connects via HTTPS to the configured host; in non-production it connects via HTTP using the configured host and port.", "Host and port values for the software-versions service are provided by application configuration and are evaluated dynamically (callable).", "API requests omit the format segment in the URL path (include_format_in_path is disabled)."]}
{"file": "app/models/service.rb", "type": "model", "domain_rules": ["A service record must be associated with an equipment (service belongs to an equipment).", "A service must include a service date and a service_due_date.", "A service must include a non-empty comment; comments that are blank or contain only HTML tags are invalid.", "Comments are sanitized before validation to remove JavaScript and stored in a safe HTML form.", "When a service's service_due_date is changed, the associated equipment's service-alarm-sent flag is reset (clears alarm notification state).", "The most recent service for an equipment (by service_due_date descending) represents the next service due for that equipment.", "Services can optionally belong to an organization and may be placed in a parent folder (both associations are optional).", "Service comments are exported sanitised (HTML stripped) when generating CSV output.", "Service records maintain an audit trail/history (changes are tracked).", "A short human-readable summary of a comment is available and defaults to truncating the comment to 20 characters."]}
{"file": "app/models/review.rb", "type": "model", "domain_rules": ["A review belongs to an organization, a team, and a reviewable entity; it may optionally reference a Procedure (when reviewable_type is 'Procedure'), an assignee (user), and a folder.", "Reviews start in a pending state and can transition through: assigned, approved, rejected, or expired.", "Pending reviews can be assigned (pending -> assigned); assigned reviews can be reassigned (assigned -> assigned).", "Only assigned reviews can be approved or rejected (assigned -> approved / rejected).", "Pending or assigned reviews can be expired (pending/assigned -> expired).", "Assignment actions record the assignment moment (assigned_at set to current UTC) when an assign or reassign occurs.", "Approval and rejection record completion (completed_at set to current UTC); completed means either approved or rejected.", "Approvals and rejections require capturing the assignee's signature (approved_sig = assignee.signature) as part of the transition.", "When approving a review for a Datum reviewable, the workflow: creates acknowledgements for each Datum warning (approved = true, using the latest note body and assignee's organization), marks the warning events as informational, and sets the Datum status to WORKFLOW_APPROVED.", "When rejecting a review for a Datum reviewable, the workflow sets the Datum status to WORKFLOW_REJECTED.", "Approval-related acknowledgement creation is transactional: failures roll back the whole approval operation.", "assignable? and reassignable? require that the acting user is authorized (ReviewPolicy.start?) and that an appropriate team queue exists for the reviewable type.", "Timing metrics are exposed: assigned_in = hours from created_at to assigned_at, completed_in = hours from assigned_at to completed_at (nil if timestamps missing).", "Notes are associated with the review and are deleted when the review is deleted.", "Changes to the review's assignee (user_id) are subject to a user privacy policy validation when the user_id changes."]}
{"file": "app/models/training_record.rb", "type": "model", "domain_rules": ["A training record is associated with a training course, a user, and an organization (must belong to each).", "A training record must have a completion date (completed_at).", "completed_at cannot be a future date (must be on or before today).", "Changing the user_id triggers a user privacy policy validation (user changes must satisfy privacy rules).", "The expiration (valid_till) is computed by adding the training course's validity period (in seconds) to the completion date; renewal month and year are derived from this expiration.", "A training record is considered active while its valid_till has not passed (valid_till >= current time).", "Associated documents are deleted when the training record is destroyed.", "The display name for a record is formed as \"<user name> - <training course name>\".", "Search for training records combines matches found via users and via training courses (aggregates user.search and training_course.search results)."]}
{"file": "app/models/project_member.rb", "type": "model", "domain_rules": ["A ProjectMember represents a membership linking a user to a specific project.", "Every ProjectMember must be associated with a User.", "Every ProjectMember must be associated with a Project; changes to the membership update the project's timestamp (touch).", "All changes to project memberships are tracked/audited (version history maintained).", "Project members are ordered by their creation time by default."]}
{"file": "app/models/task_create_consumable.rb", "type": "model", "domain_rules": ["A TaskCreateConsumable must have a reference, and that reference must be unique within the same procedure.", "The task may be associated with an existing Consumable (optional); the consumable association is stored on the task and referenced when present.", "The task may optionally reference a ConsumableMaster.", "Additional business constraints for creating or associating consumables are enforced by a custom CreateConsumableValidator.", "When resolving the associated object by version id, the task returns a reified historical version of the consumable unless the requested version has a later version whose changes include certificate_of_analysis_id — in that case the current consumable is used instead.", "If a requested associated version cannot be found, a RecordVersionNotFound error (with the consumable and version id) is raised.", "The task's outcome includes an editable link to the associated consumable (if present) combined with the task outcome, and the resulting HTML is sanitized to prevent unsafe markup."]}
{"file": "app/models/result_xcellerex.rb", "type": "model", "domain_rules": ["This model stores Xcellerex measurement results and uses the timestamp column as the implicit ordering key (chronological ordering).", "Imports only accept CSV files (.csv); any other file extension marks the import as unsupported, sets the import datum's end_time to now, and creates an 'error' Event tied to the equipment, datum and organization with an 'unsupported_file_type' comment.", "CSV imports require the timestamp in the first column and map specific whitelisted CSV headers to sensor attributes (timestamp plus brx01_* fields such as pressure PT001, temperature TT001, AT001–AT004 variants, weight WT001A/WT001B, flow controllers FC001–FC005, speed controllers SC001–SC007, etc.).", "Only the predefined set of CSV_WHITELISTED_FIELDS are considered for import (the class exposes csv_whitelisted_fields and csv_fields_map for this purpose).", "All sensor values from CSV are parsed to floats when building results (non‑numeric or missing values are converted via to_f, effectively yielding 0.0 if blank).", "After a successful import the import_datum record is updated: name is set to the imported file's basename, start_time is set to the minimum timestamp among the imported results, and end_time is set to the maximum timestamp among the imported results.", "Import is considered successful if any ResultXcellerex records are associated with the import_datum after import (import_file returns truthy only when results exist).", "After saving imported results, the model refreshes the updated_at timestamp on the first and last chronological results to trigger any downstream after_save callbacks or processing.", "Unsupported-file handling is explicit and auditable: an Event record (event_type 'error') is created with equipment_id, datum_id, organization_id and a localized comment when a non-CSV file is encountered."]}
{"file": "app/models/formula.rb", "type": "model", "domain_rules": ["Applying (freezing) formulae updates the task's stored formula values and returns the updated task plus any associated objects that changed.", "When a task's completion status changes from false (or nil) to true, applying formulae may also update related/associated objects — but only if modification of associated objects is permitted (can_modify_associated_object).", "Resetting formulae replaces displayed formula values with the placeholder '--' in task/question details and returns the updated entity plus any associated objects that changed.", "When resetting, associated objects are only reassigned if the task has a last_completed_at timestamp, the task's completion is changing to false (or nil), and can_modify_associated_object is true — the reassignment is performed with reference to the state before last_completed_at.", "Question-level formulae only affect spans inside the question's HTML detail whose data-formula-ref matches the formula ref and whose question_ref matches the question's reference.", "Freezing at the question level writes the provided formula value into the matching HTML fragments; resetting sets those fragments' content to '--'.", "All operations return both the primary entity (task or question) and a list of associated objects that were changed as a result of applying or resetting formulae.", "Modification of associated objects is explicitly gated by a can_modify_associated_object flag in the input parameters.", "Associated-object updates accept instructions embedded in formula data (e.g., save_to target columns/task identifiers) and are delegated to the Task-level assignment logic.", "Only transitions in the task's completion attribute trigger conditional associated-object updates: specifically transitions to true trigger freeze-side updates; transitions to false/nil (with last_completed_at present) trigger reset-side reverts."]}
{"file": "app/models/extractor_profile.rb", "type": "model", "domain_rules": ["An ExtractorProfile owns many segments; deleting the profile removes its associated segments.", "Segments are validated together with the profile (all associated segments must be valid to save the profile).", "Nested creation/updating/deletion of segments is supported via nested attributes (segments can be created/updated/destroyed through the profile).", "A profile cannot have more than 20 segments; exceeding this adds a validation error (profile_exceeds_20_segments).", "The profile’s approver is the whodunnit of the most recent version; if there are no versions, approver is nil.", "A profile MD5 checksum is computed from the profile name, description (with fallbacks), and each segment’s MD5 — i.e., changes to name, description or any segment alter the profile checksum.", "Real-time serialization (to_cable_object) exposes the profile id, name, class name and each segment’s id, end_time, and end_temp for cable communication.", "Segments are associated via a custom foreign key (freezing_profile_id), indicating these segments are tied specifically to the profile’s freezing context."]}
{"file": "app/models/change_notification.rb", "type": "model", "domain_rules": ["Change notifications represent records of changes that require approval and distribution to relevant teams within an organization.", "Every change notification belongs to an organization.", "Every change notification must target a team to notify (team_to_notify is required).", "Each notification must include a summary and detailed description (summary and detail are required).", "Notifications have a draft flag; a notification is considered approved/published when draft is false (approved? => !draft).", "An approver (approved_by) is optional, but if the approved_by_id is changed it must satisfy the user privacy policy validation.", "Notifications can be linked optionally to domain entities: manufacturing process, procedure, profile, consumable master, and label.", "Associated documents are not deleted when a notification is removed; their association is nullified (documents retain records but lose the link).", "Notifications are implicitly ordered by creation time (created_at) by default.", "Search respects user authorization (Pundit.policy_scope) and supports case-insensitive partial matching on summary; when no search term is given the result is limited (default max 5)."]}
{"file": "app/models/subscription_plan.rb", "type": "model", "domain_rules": ["A subscription plan must have a name.", "A subscription plan can have many subscriptions; a plan cannot be deleted while it still has associated subscriptions (deletion is restricted and will produce an error).", "Changes to subscription plans are versioned/audited (paper_trail) so modifications are tracked (updated_at is ignored for version diffs).", "Before saving, the plan normalizes its usage data: if a 'users' entry exists in the usage hash it is converted to an integer.", "The usage data may be nil and is handled gracefully during normalization.", "Subscription plans are implicitly ordered by their creation timestamp (created_at) when relying on default ordering."]}
{"file": "app/models/sensor.rb", "type": "model", "domain_rules": ["A sensor represents an environmental measurement attached optionally to equipment; it may exist without being assigned to equipment.", "Sensor types are fixed and enumerable: temp, humidity, co2, channel_1_particle_count, channel_2_particle_count, channel_3_particle_count, channel_4_particle_count, channel_5_particle_count, and channel_6_particle_count.", "Each sensor type carries metadata (title, subtitle, icon, display order, unit, slider_type, precision) and has configured alert thresholds used for status evaluation.", "A sensor's current value is sourced from its equipment's latest measurement record (if equipment is present); if the value is not numeric the sensor alert status is unknown.", "Sensors can be marked active/inactive and are typically filtered by an active scope when listing active sensors.", "When a sensor is deleted its associated events are preserved but disassociated (event foreign keys are set to null).", "Alert status is derived by comparing the current numeric value against sensor-specific warning and error thresholds: values outside error thresholds yield 'error', outside warning thresholds yield 'warning', otherwise 'ok'.", "For sensors with slider_type 'double' (two-sided sliders) both lower and upper thresholds are evaluated; for 'single' sliders only upper-threshold checks are treated for low-bound conditions.", "The mapping of sensor_type to an integer ordering enforces the allowed types and their display order.", "Human-readable full titles (including units) are provided per sensor name via localization; unknown names map to 'Unknown'.", "Default ordering for sensor collections is by creation time unless otherwise specified."]}
{"file": "app/models/task_select_supplier.rb", "type": "model", "domain_rules": ["The task may reference a Supplier (association stored in the task's 'value' field); that supplier association is optional (a task can have no supplier selected).", "Each task must have a 'reference' and that reference must be unique within the same procedure (no two tasks in a procedure share the same reference).", "When resolving the associated object, if a supplier and an associated_object_version_id are present, the system looks up that supplier version; if that version has a subsequent ('next') version it returns the next version's reified state, otherwise it falls back to the current supplier.", "If the requested supplier version cannot be found, a domain-level Exceptions::RecordVersionNotFound(supplier, version_id) is raised (missing version is treated as an error).", "The task outcome, when an associated object exists, includes a sanitized edit link to the supplier's edit page prepended to the task's existing outcome (enables navigation from the task to edit the supplier)."]}
{"file": "app/models/datum.rb", "type": "model", "domain_rules": ["A Datum always belongs to an Organization; many other associations (equipment, user, product, project, facility, batch, operator, verifier, folder, profile) are optional.", "A Datum must have a name.", "User, operator and verifier assignments are subject to a user privacy policy check when changed.", "Associated product and associated equipment relationships are validated by domain validators before saving.", "If no project or facility is set on create, they are automatically inherited from the associated equipment, product or the creating user.", "If no associated equipment IDs are provided on create, the primary equipment and its linked equipment are automatically associated.", "Comments are sanitized to remove JavaScript before validation.", "Status is auto-managed: set to 'Completed' when end_time is present, otherwise 'In progress' — except for equipment marked as manual_trigger (those skip automatic status handling).", "When a Datum is created the system fires 'datum_created' webhooks for the organization; when an end_time is set it fires 'datum_completed' webhooks (skipped for manual-trigger equipment).", "When a Datum first transitions from no end_time to having an end_time and status is 'Completed', associated tasks are completed/updated and their state is broadcast to procedure channels.", "Completing associated tasks attaches the Datum id/version to the task, marks tasks complete or failed, updates project/facility when triggered from a task, and can create deviations for warnings/errors.", "If a task has validation warnings or errors, the Datum completion process will create task deviations summarizing those warnings/errors; if the Datum contains warnings/errors it will also create a deviation (unless one already exists).", "A Datum requires review if it has any warning events or chronicle error events; warnings and errors are derived from the Datum's events stream.", "Measurement-derived attributes (load_temperature, final_temperature, final_co2_level, final_ph, final_mass, initial/final volume, live/total cells, number_of_detectors, total_volume, number_of_fractions, maximum_rpm, donation_identification_number) are computed based on equipment type and result records — different equipment types expose and use different measurement fields.", "Summary output for a Datum is equipment-type specific and returns the most relevant measurement(s) for that device (eg. temperature for freezers, volumes for Sepax2/Sefia, counts for cell counters, mass/pH for XuriWave, etc.).", "Results ingestion (add_results) maps incoming measurements into Result fields according to equipment device_type (different field mappings for ViaFreezeQuad, ViaFreezeDuo/Freeze, ViaThawCryobags/L1000, etc.), and the last result triggers result-level webhooks.", "Exported CSV formats are customized per instrument/device type; some instruments are excluded from the extended export and freezer equipment triggers a background freezer-log CSV generation job when completed.", "Deleting a Datum cascades or nullifies related data according to domain rules: measurement results, I/O params and specialized result sets are removed, events/documents/reviews/linked_data removed, and task_sop_data references are nullified.", "Workflows: Datum instantiates defined workflows (currently EquipmentReceivedWarningWorkflow) used to run equipment-received checks and to provide a review_workflow instance for UI/processing.", "Duration and timing: human-readable duration and duration-in-seconds are calculated from start_time (or first result timestamp) to end_time (or last result timestamp), falling back to '-' or 0 if timestamps are missing.", "Formula field rules and permitted read formula fields are predefined lists — certain named numeric and text fields are recognized for formula validation and reading within the domain.", "Search and listing behavior supports filtering by equipment, product, project, facility, folder, tags, and free-text search across name, comment, times, equipment and product identifiers, with timezone-aware time matching."]}
{"file": "app/models/result.rb", "type": "model", "domain_rules": ["A Result is a time‑stamped measurement for a Datum and records are implicitly ordered by logged_at.", "CSV import and field mapping are device‑specific: different whitelisted fields and CSV→domain mappings are used for thawer devices, freezer devices (including Duo/Quad variants), and the default device type.", "Thawer devices (ViaThawCryobags and L1000) use a thawer CSV workflow; L1000 adds extra fields (result22, result25) to the allowed CSV fields.", "For ViaThawCryobags/L1000 CSV imports the record is handled via the ViaThawCryobag import flow and can trigger an alert operation when import succeeds.", "Freezer devices only accept CSV files for automated parsing; non‑CSV uploads stop automated parsing and set the datum end_time to now.", "Freezer CSV parsing expects initial metadata lines (used to set datum.name, datum.comment, and datum.start_time from specific line positions) before the measurement rows.", "Freezer CSV measurement rows use 'Elapsed time (seconds)' to compute logged_at as datum.start_time + elapsed seconds, and map temperature and thermocouple columns into result1..resultN; ViaFreezeQuad provides many more thermocouple/result fields than Duo/other freezers.", "After any successful import the datum start_time and end_time are updated to the minimum and maximum logged_at among its Results (or set to now if there are no results).", "If an equipment is a freezer and the organization's digestor_feature is enabled, a freezer-with-federer whitelist (including f_set_speed, f_actual_speed, f_tc1) is used instead of the normal freezer whitelist.", "producttemp returns result5 for ViaThawCryobags devices; for all other device types it returns result1.", "to_aliased_attributes provides standardized, device‑specific names for measured values (e.g., current_temperature, setpoint, thermocouples, left/right plate temps, pv/op/ir fields) so downstream code can treat different devices uniformly.", "avg_top_heater computes the average of available top‑heater PV sensor readings (result23, result2, result24) and returns nil if none are present.", "avg_bottom_heater computes the average of bottom heater PV readings (result1 and result25) and returns nil if both are absent.", "A successful freezer CSV import delivers webhooks for the last created Result (noting results_skipped in that notification)."]}
{"file": "app/models/accessory.rb", "type": "model", "domain_rules": ["Accessory has a device_type drawn from the application's configured accessory device types (enum).", "An Accessory may optionally belong to a parent Equipment; accessories can exist unattached.", "If an accessory is attached to equipment, the attachment is validated: the parent equipment's device_type must be allowed for this accessory's device_type according to the configured device_attachments mapping.", "If an accessory's device_type is not present in the device_attachments mapping, attachments are permitted by default.", "On creation, an accessory inherits its parent equipment's facility_id and project_id when those are present (copied immediately, bypassing normal validations).", "list_equipment_can_attach_to(equipment_type) encodes allowed attachment relationships for specific equipment types (examples: Digestor => ViaFreeze; XuriPump/XuriCBCU => XuriWave; SefiaSelect => Sefia; Probe => all configured storage types; Other => storage + instrument types).", "associated_equipment returns the parent equipment plus that equipment's other accessories (excluding the accessory itself), i.e., the primary equipment and its sibling accessories.", "When an invalid attachment is detected, an error is added to equipment_id with a localized message indicating the accessory is not a valid device attachment."]}
{"file": "app/models/user.rb", "type": "model", "domain_rules": ["Users must have a name and role.", "Passwords are required for new users or when password fields are provided; password confirmation and length rules apply.", "Device accounts are special: they do not require email and display device serial numbers instead of normal usernames.", "Only certain roles exist (observer, operator, approver, admin, api); api and device roles do not have UI access.", "A user’s role cannot be changed to or from 'api'.", "Email is required for non-device users; SSO/Omniauth sign-ins require a matching SSO provider for the user’s email.", "SSO-created/updated users are assigned to the SSO provider’s organization, may have roles mapped from SSO group IDs, and can be blocked from creation/update when subscription rules fail.", "Users must accept the privacy policy on update unless that validation is explicitly skipped for the operation.", "Workday configuration must be valid: start and end are numeric minutes in a day (0..24*60), workdays count >= 0, and workday_timezone must be a valid TZInfo identifier.", "Users may belong to an organization and many domain entities (teams, projects, facilities, training records, tasks, etc.); team/project/facility associations are removed if they belong to a different organization when organization changes.", "Subscription constraints apply on create/update: organization active subscription must not have exceeded user quota, must not be fully expired (grace period exceeded), and an active subscription must exist or a warning is added.", "Organization subscription user-limit checks skip users who are subscription base-level users (including special-case allowance for single-base-level API user).", "If subscription validations fail during SSO/omniauth provisioning, specific subscription errors are raised (user limit exceeded, has expired, ended warning).", "Admin locking disables access immediately: admin_locked true invalidates sessions and locks account; admin unlock attempts to restore access only within successful subscription and privacy validations.", "Session management: each user has a session token used to uniquely salt authentication and clearing session_token invalidates sessions.", "Users internal to the company are identified by ge.com or cytiva.com email domains.", "UI visibility: ui_access? returns true for roles other than api and device.", "Training/attendance and workday selections are tracked; users have training records and active attendance is checked against training courses.", "CSV export and search behaviors exclude device-role users from normal user searches and respect Pundit policy scope.", "An override token (short hex) is assigned to new users (used for SSO override scenarios)."]}
{"file": "app/models/task_print_product_label.rb", "type": "model", "domain_rules": ["This task represents printing a product label and is responsible for label-related validations and outcome sanitization.", "A reference value is required for the task and must be unique within the same procedure.", "The task may be linked to a Label, but the label association is optional.", "A label is considered approved for this task only if it is approved, belongs to the same organization, and is of type 'product'.", "Task outcome text is prefixed with a localized 'complete' or 'incomplete' status and is sanitized to remove unsafe content.", "The task derives its associated business object via its reference task (i.e., it delegates to the referenced task's associated object)."]}
{"file": "app/models/software_version.rb", "type": "model", "domain_rules": ["SoftwareVersion supports bulk creation of records via an authenticated JSON API endpoint.", "Bulk create payload must be an array of version objects provided under the top-level key 'software_versions'.", "Caller may specify a custom collection name for the endpoint; default is 'software_versions'.", "Requests must be authenticated using configured basic-auth credentials (email and password) from application config.", "The API is expected to accept and return JSON and to conform to the v3 vendor media type ('application/vnd.todos.v3+json').", "A successful bulk operation is indicated by HTTP 200 or 201; any other response is treated as a bulk update failure.", "Bulk update failures raise BulkUpdateFailed (meaning changes were not applied to all records) and are logged.", "On success the bulk create method returns true; failures are caught and logged (no further exception propagation)."]}
{"file": "app/models/equipment.rb", "type": "model", "domain_rules": ["Every equipment belongs to an organization and must have a type, a device_type and an associated user.", "Equipment name must be present and unique within the same organization among active equipment.", "Serial number (serialnum) must be present at creation, contain no spaces, and be unique among active equipment; additional serial validations apply via a custom validator.", "Certain device types require a hostname: when device is active, hostname present, and device_type is in the HOSTNAME_DEVICES list; hostname must contain no spaces, be ≤15 chars and unique among active equipment.", "unicorn_laptop_hostname is required for OPC UA-enabled equipment when OPC UA is required or for mRNA AKTA devices.", "Equipment condition must be one of the predefined condition enums; cleaning_status, refrigerant, temperature and particle_limits are constrained to their respective enumerated values.", "Equipment must have a valid time_zone drawn from ActiveSupport::TimeZone mappings.", "Equipment must belong to an organization member of the facility/project according to FacilityProjectMemberValidator and associated facility/project constraints enforced by custom validators.", "Removing equipment cascades to remove or nullify associated domain records: data, results, services, therapy_images, documents, reservations, events, sensors, specialized equipment logs and the associated user (user is destroyed when equipment archived/removed).", "Equipment can optionally belong to project, facility, reviewer_team, escalation_team or a parent folder; accessories (associated equipment) are considered children and moved to equipment's facility/project when the parent changes.", "LDAP configuration is device-type specific; LDAP templates exist for certain device types and LDAP is considered enabled when ldap_config is present. LDAP engineer mapping and activation depend on self-service capabilities for the device.", "Profiles can be auto-assigned based on device_type using DEVICE_TO_PROFILE_MAP; assignable profiles are restricted by device family rules (e.g., freezers, extractors, CB1000, L1000) and must be approved and belong to the equipment's organization.", "Some device families support a Software Version Service (SVS); SVS is only available for device types in SOFTWARE_VERSION_SERVICE_SUPPORTED_EQUIPMENTS. Requesting package updates triggers a software-version update callback.", "Package/software update requests are processed only for thawer/freezer families or SEFIA family devices; requested version fields (firmware, OS, application, software) or a selected_version_changed flag trigger the update flow.", "If requested firmware/software versions change for thawer or freezer equipment, a custom validator ensures requested versions are valid before updates are performed.", "Next service date is determined from Service.next_service_due_by_equipment or a legacy date; when both exist the later date is used. Equipment is considered in need of service if next_service_due_date is within the next two months.", "A service alarm is sent only once until reset: send_service_alarm? is true when needs_service? is true and service_alarm_sent is false; reset_service_alarm_sent clears the flag.", "Equipment can be archived: archiving marks it inactive and removes its associated user inside a transaction; failures roll back and archive reports failure.", "Reservations overlap detection: reserved_by_others? returns true when there exists an overlapping reservation at present that was not made by the current user or any of their teams.", "Device-type related boolean helpers determine capabilities and classification (e.g., freezer?, thawer?, akta?, chromatography?, sefia?, sepax2?, monitoring_device?, gateway?, cytiva_equipment?, manual_trigger?).", "Status calculation: for monitoring devices the maximum of sensor alert statuses and latest datum status determines alert; for other devices latest datum dictates warning/error/ok, otherwise online state maps to ok or unknown.", "Events acknowledgements are counted for the last 24 hours for equipment-related data to drive alert/acknowledgement metrics.", "PIN code generation for CB1000-like units is deterministic per-day: a 4-digit PIN derived from serial number, current date and a server secret.", "Equipment requiring a gateway is determined by membership in the GATEWAY_DEVICES list; some manufacturer details are considered static for specific device families (ExpertGTX and Cytiva equipment).", "Catalog number is editable only for NanoAssemblr, AKTA family, or Xcellerex devices.", "Hostname validation and other conditional behaviors are governed by device-type membership lists (HOSTNAME_DEVICES, MANUAL_TRIGGER_DEVICES, BIOSAFE_DEVICES, GATEWAY_REST_API_DEVICES, etc.).", "Geocoding runs before save when location changes; latitude/longitude are exposed when serializing JSON if requested.", "Custom validators enforce domain constraints beyond standard validations: attachments, associated facility, LDAP config, and requested software versions for thawer/freezer equipment."]}
{"file": "app/models/task.rb", "type": "model", "domain_rules": ["A Task belongs to an Organization and optionally to a User, a Verifier (verified_by), a Procedure, and a Folder.", "A Task must have a name.", "A Task reference must be unique within its Procedure (when procedure is present).", "Only certain Task types are allowed (e.g., TaskInstruction, TaskQuestion, TaskPicture, TaskQRCode, TaskCreateConsumable, TaskMultipleQuestions, etc.); some types may be excluded by policy (e.g., MRNA).", "Question types are restricted to: Text, Number, Multi-choice, Date.", "For TaskLoop type, loop_until_ref and task_conditional are required.", "When a Task is marked complete (or its value changes), last_completed_at is set; the first time a Task becomes complete (last_completed_at changes from nil) will trigger starting the next procedure for the associated batch if the Procedure has a batch.", "If a Procedure is a template, embedded images in the Task detail are uploaded to S3 on save.", "TaskUploadDocument (non-template) that has its value changed to a non-blank value and requires a document number will create a Document for the Organization; failure to create the Document records a Task deviation.", "Tasks evaluate custom JSON conditions/rules on completion (unless override_validations): matching conditions can set requires_verification = true or mark the Task as a deviation and push warning messages. For TaskMultipleQuestions rules are evaluated per question.", "Allowed deviation outcomes (DECISION list) include: lookup_manually, supervisor_override, data_contains_warnings_link_here, more_time_was_needed, invalid_record.", "Formulas embedded in Task detail/questions can write calculated values to other tasks' associated objects, but writes are restricted to PERMITTED_WRITE_FORMULA_FIELDS; attempts to write unauthorized fields raise an error. Special handling exists for components_* and quantity fields.", "When resetting Task data, the Task reverts value, completion, verification, tags and may restore associated object versions; if no other tasks reference the associated object and it supports locking, the lock is released. Some associated object types are not resettable (TaskWebhook, TaskQRCodeEquip, TaskSopDatum).", "Tasks may reference other tasks by reference_to within the same Procedure and Organization. For tasks in loops, references resolve to the specific loop iteration and referencing across loop boundaries is prevented.", "Only QR-related tasks (TaskQRCode, TaskQRCodeConsum, TaskQRCodeEquip) may have validation errors that are considered overridable; an Equip QR with a scanned_wrong_type base error cannot be overridden.", "Tasks have many Notes, Deviations and Tags; these associated records are destroyed when the Task is destroyed.", "Webhook-related attributes are hidden from APIs/serializations for all Task types except TaskWebhook.", "Saving formula values to associated objects can be performed programmatically (assign_formula_values / assign_formula_values_to_associated_objects) and will collect and persist affected objects; quantity updates split value into number and unit."]}
{"file": "app/models/freezing_profile.rb", "type": "model", "domain_rules": ["A FreezingProfile is composed of many segments; segments are managed together with the profile and are deleted if the profile is deleted.", "Segments associated with a FreezingProfile must be valid (validations on segments are enforced when saving the profile).", "Segments can be created, updated, or removed through nested attributes on the profile (nested segment management is allowed, including destruction).", "Approval/auditing is versioned: the profile's approver is taken from the most recent version's whodunnit, if any versions exist.", "A profile's integrity/change fingerprint (MD5) is derived from the profile's name, description, and the MD5s of all its segments — used to detect changes.", "When serializing for cable delivery, the profile exposes its id, name, class, and for each segment: id, end_time, and end_temp."]}
{"file": "app/models/document.rb", "type": "model", "domain_rules": ["Every document belongs to an organization and is scoped/numbered per-organization.", "A document must have a name and a file upload.", "Documents are automatically assigned a sequential document_number on create based on the organization's last_document_number and the current maximum document_number for that organization.", "A document's document_prefix is set from the organization's document_prefix on create; any '<TYPE>' placeholder in a provided prefix is replaced with 'DOC'.", "If a created document's number is greater than the organization's last_document_number, the organization's last_document_number is updated to that number.", "For users with role 'approver' or 'operator', project_id and facility_id are auto-filled from associated equipment, datum, consumable, or product when those IDs are not provided.", "Associated product, equipment, and datum must belong to the same organization as the document (validated).", "The notes field is sanitized to remove embedded JavaScript (prevents XSS) before validation.", "A document exposes a combined document_prefix_and_number (prefix + zero-padded 4-digit number) when document_number is present and includes it in JSON output and default queries.", "Documents are versioned (audit trail) and changes to updated_at are ignored for versioning.", "Removing a document nullifies links on related records (e.g., certificate_of_analysis on Consumable and import_job) rather than deleting those associated records.", "A facility/project membership validator is enforced (FacilityProjectMemberValidator) to ensure project/facility relationships comply with membership rules.", "Search for documents is permission-scoped and matches name, stored filename, or the prefix+number string."]}
{"file": "app/models/result_biostore.rb", "type": "model", "domain_rules": ["Each record represents a timestamped biological data point and is ordered by its logged_at timestamp by default.", "Core domain attributes include logged_at, item_name, and item_value; these represent the time, the measured item, and its measured value.", "CSV export is restricted to a whitelist: only logged_at, item_name, and item_value are exported (all other attributes are excluded).", "There is no CSV field remapping configured (CSV_FIELDS_MAP is empty), so exported column names match the attribute names directly.", "The model includes shared result behavior via the DatumResults module, meaning it relies on common result-processing logic defined elsewhere."]}
{"file": "app/models/task_multiple_questions_question.rb", "type": "model", "domain_rules": ["Represents a configured question that belongs to a multi-question task (a single question instance within a multi-question configuration).", "A question has these domain attributes: question_type, question_options, detail, conditions, reference, and sequence.", "When created with a question hash, the instance will populate question_type, question_options, detail, conditions, and reference from that hash.", "Sequence is a separate attribute used to indicate the question's order/position and can be provided independently at initialization.", "question_options captures the selectable choices or options for the question; conditions capture rules or criteria that govern when or how the question applies.", "reference holds a reference identifier or link associated with the question (e.g., to external data or documentation).", "The instance exposes an ActiveModel::Errors object to record validation or business-rule errors for the question."]}
{"file": "app/models/import_job.rb", "type": "model", "domain_rules": ["Only specific models can be imported: Supplier, StorageUnit, Storage, Consumable, ConsumableMaster, Clinic, Product, and TrainingRecord.", "An import job must belong to an organization.", "An import job must specify the model to import and that model must be one of the whitelisted models.", "The import references an uploaded import file (a Document) and the file must exist and be a CSV or plain-text file (otherwise the import is invalid).", "A user can be associated with an import job (optional), and changes to user_id are subject to a user privacy policy validation.", "Import jobs track many import_records; deleting an import job removes its import_records.", "Import jobs maintain a state machine with these states and transitions: pending → enqueued (enqueue), enqueued → in_progress (start), in_progress → successful (success), in_progress → failed (fail).", "Headers and total record count are extracted from the model-specific importer; records_count drives progress calculations.", "Progress is reported as a percentage of processed import_records versus records_count, defaulting to 100.0% when records_count is zero.", "If the import headers include 'ID', the import is treated as an override (i.e., existing records may be updated).", "The importer used is a model-specific class named by appending 'Importer' to the model name and is only instantiated when the import job has a valid model and the record is valid."]}
{"file": "app/models/result_one_edge.rb", "type": "model", "domain_rules": ["Each ResultOneEdge must have a tag_name (tag_name is required).", "A ResultOneEdge represents a single-edge tagged result tied to a datum (the datum may carry nested events).", "Results are implicitly ordered by created_at by default.", "CSV export only exposes whitelisted model fields (currently tag_name) plus any dynamic keys found in each result's data payload; the CSV header is the whitelisted fields followed by the unique union of all data keys across results.", "When exporting rows, the exporter prefers model attributes; if an attribute is missing it falls back to the corresponding value inside the result's data hash.", "Time values in CSV output are formatted as ISO 8601 timestamps with millisecond precision.", "If the associated datum has events, those nested events are appended to the CSV via their own export."]}
{"file": "app/models/equipment_log.rb", "type": "model", "domain_rules": ["All models related to equipment logging are grouped under the EquipmentLog namespace.", "Tables that persist equipment log data are named with the prefix 'equipment_log_', ensuring equipment log records are stored in a dedicated set of tables.", "Any model defined inside the EquipmentLog namespace will use the 'equipment_log_' prefix for its database table name, separating equipment log data from other domain data."]}
{"file": "app/models/thawer.rb", "type": "model", "domain_rules": ["Thaw events are identified by a numeric code and human-readable message; each event can be flagged as a warning or as privileged.", "A known set of event codes (1..27) map to specific domain meanings (e.g., 1 = User loaded the product, 5 = Thaw complete, 19 = Lid Opens, 20 = Lid Closes).", "Events that represent warnings: codes 2, 3, 6, 24, 25, 26, and 27 — these indicate conditions requiring operator attention (cancel, TOO WARM, time threshold, alarm, lid open during run, safety interlock, mains power loss).", "Privileged events indicate end-of-thaw (EoT) causes and outcomes: codes 7–18 (IR/Time/Energy/Stopwatch triggers, erroneous thaw, and crystal outcomes 12–15).", "Crystal outcome codes: 12 = Small Crystal, 13 = Large Crystal, 14 = No Crystal, 15 = Unacceptably Large Crystal — these are privileged EoT results.", "When counting warnings, only entries whose mapped Event has its warning flag set are counted; nil or missing entries are ignored.", "Footer lines are expected in 'EpochTime EventCode' form; the parser extracts the time and numeric code and maps the code to a known event. Lines without a present time or without a known event mapping are discarded.", "Header strings are comma-separated and mapped to domain fields: first field = Name, second = Fill Volume, third = Overwrap (Overwrap is true only when the third field equals '1').", "Body content, when present, is parsed as CSV with headers; a nil body yields an empty result set.", "Unknown or unmapped event codes in the footer are treated as absent (events are filtered out unless both Epoch Time and Event mapping are present)."]}
{"file": "app/models/error_report.rb", "type": "model", "domain_rules": ["An ErrorReport encapsulates an error message and an associated backtrace for consistent frontend error reporting.", "Initializer requires a message and a backtrace (backtrace may be nil, a String, or an Array).", "If backtrace is provided as a String it is normalized into an array of lines.", "Calling raise_front_end_error always raises a FrontEndError containing the stored message.", "If a backtrace is present (non-empty), raise_front_end_error attaches the normalized backtrace to the FrontEndError.", "FrontEndError is a custom subclass of StandardError used to represent frontend-layer errors for targeted handling."]}
{"file": "app/models/subscription.rb", "type": "model", "domain_rules": ["A subscription belongs to an organization and an owning user; it may reference a subscription plan and stores plan details at the time of assignment.", "started_at, ended_at, csm_name and csm_email must be present.", "started_at must be before ended_at; ended_at cannot be set to a past date when creating/updating.", "Subscription periods for the same organization must not overlap.", "A subscription is considered active when started_at <= now <= ended_at; future when started_at > now; past when ended_at < now.", "Once a subscription has ended (past), its end date cannot be modified and the record cannot be edited in ways disallowed by validation.", "Active or previously active subscriptions cannot have their start date, end date, organization, or plan changed.", "A subscription cannot be made active via update if it was not previously active (activating an inactive record is disallowed).", "Subscriptions cannot be destroyed if they are currently active or have already ended.", "Fully expired means either there is no end date or the grace period after ended_at has elapsed; the grace period length is provided by application configuration.", "A subscription’s grace_period_expired is true when it is fully expired and the organization has no other future or active subscription records.", "A subscription is considered renewed if it is not future and there exists another future or active subscription for the same organization.", "Renewal notifications follow an ordered lifecycle (six_month, three_month, one_month, one_week, one_day_expired, weekly_expired, three_month_expired, grace_period_expired) and transitions are guarded to prevent duplicate sends; some early notifications require the subscription to be active.", "Notifications are suppressed and marked not_required when a subscription has been renewed or when grace-period expired notifications are not allowed.", "Which notification is due is determined by ended_at relative to now (e.g., <=6 months, <=3 months, <=1 month, <=1 week, within 1 day after expiry, 1 day–3 months after expiry, grace-period expired) and the current renewal_notification_status guards.", "Subscription plan details are copied into the subscription when the associated plan is changed, preserving any unsaved details if present.", "Usage-derived user limits come from the subscription plan usage (users count), with +5 seats for GMP instances; base-level users (owner and earliest API user) can be optionally included in the limit.", "Active users for quota checks are organization users in roles admin, approver, operator, observer, api, excluding Cytiva accounts and optionally excluding admin-locked users, ordered by creation date.", "The subscription state resolves to :future, :past, or :active; it is :past if fully expired or another active subscription exists for the organization, otherwise :active."]}
{"file": "app/models/batch.rb", "type": "model", "domain_rules": ["A batch must belong to an organization and must reference a manufacturing process (mfg_process).", "batch_ref is required and must be unique within the same organization; batch_ref is used as the batch name/string representation.", "A batch must be assigned first, second and third reviewer teams (those reviewer team associations are required).", "Project and facility associations are optional but when present must satisfy FacilityProjectMemberValidator (i.e., membership/permission constraints).", "On creation, all 'start_after' task references declared by the mfg_process must correspond to existing task references from the process procedures; creation is invalid if any referenced task is missing.", "Procedures belong to a batch and are destroyed if the batch is removed; batches aggregate procedures to determine lifecycle state.", "A batch is considered ended only if there is at least one non-template procedure and every non-template procedure has a finalized_at timestamp (i.e., all non-template procedures finalized).", "A batch is considered in progress when it is not ended (end_time is '---' when still in progress).", "end_time is the finalized_at timestamp of the most recently finalized procedure; start_time is the batch creation time.", "Warnings for a batch are raised when any associated procedures have deviations.", "last_procedure is determined by the highest batch_sequence among procedures; last_finalized_procedure is determined by the most recent finalized_at.", "Batches support review workflows (including a ReviewBatchWorkflow) and expose the review workflow for review-related operations.", "Approval-related behavior: approved currently reports false by default; approve_by_user! performs approval via superclass behavior and immediately persists the change.", "Batches can have notes, acknowledgements, reviews and tags attached (these are domain attachments and are removed when the batch is deleted).", "Batches are auditable and relation changes to reviewer teams, manufacturing process, project, and facility are tracked for history/metadata purposes.", "Search and export: batches can be searched by batch_ref or donor_ref (subject to policy scope) and exported to CSV including batch_ref and product_name.", "UI behavior: batch toast timeout (notification display) is defined as 120 seconds."]}
{"file": "app/models/team.rb", "type": "model", "domain_rules": ["A team belongs to an organization and must have a name; team names must be unique within the same organization.", "A team represents a group responsible for reviewing, approving, and escalating resources and for receiving notifications.", "Teams are associated with many domain resources (procedures, reviews, facilities, equipment, batches, processes, equipment reservations, training courses, profiles, consumable masters, change notifications).", "When a team is removed, most resource records remain but their association to the team is cleared (team foreign keys are nullified), so resources become unassigned rather than deleted.", "Team membership records (team_members) are destroyed when the team is destroyed, removing the association between users and the team.", "A team can act as reviewer_team or escalation_team for facilities and equipment, as first/second/third reviewer for batches, and as approval_team for training courses, profiles, and consumable masters; those role assignments are cleared if the team is deleted.", "A team is invalid unless its users have accepted the privacy policy — privacy acceptance for associated users is validated and any violations are reported on the team record.", "Teams provide a users_count and a small sample of up to five users (name and email) for display purposes.", "Team records are versioned/audited (paper trail) to track changes over time."]}
{"file": "app/models/protocol.rb", "type": "model", "domain_rules": ["A protocol must have a name.", "A protocol must have a numeric identifier (num).", "A protocol must specify exactly one version: either an integer 'ver' or a string 'ver_string' — one is required and both cannot be present.", "Within the same num/version combination, protocol names must be unique (uniqueness enforced on create: scope is name+num+ver when ver is present, or name+num+ver_string when ver_string is present).", "A protocol can have many protocol variables and many profiles; deleting a protocol removes its associated variables and profiles.", "Protocols can be marked active; an 'active' scope selects only active protocols.", "Editable protocol variables are retrieved in ascending creation order via the provided helper (i.e., editable variables are ordered by created_at).", "When needing a version label, the integer 'ver' is preferred and returned if present; otherwise the 'ver_string' is used."]}
{"file": "app/models/result_akta_ready450_volume_hda.rb", "type": "model", "domain_rules": ["Represents AKTA Ready 450 HDA measurement results tied to a datum, an item name and an item volume.", "item_value must be numeric.", "There can be at most one result for a given combination of datum_id, item_name and item_volume (uniqueness enforced).", "CSV imports/exports are restricted to the fields: item_volume, item_name, item_value.", "No custom CSV field mapping is provided (CSV_FIELDS_MAP is empty).", "Includes shared datum-result behavior (via DatumResults), implying association/processing tied to a datum context."]}
{"file": "app/models/via_thaw_cryobag_result.rb", "type": "model", "domain_rules": ["Represents the outcome/result of a cryobag thaw operation.", "Each ViaThawCryobagResult is associated with the specific viaThawCryobag thaw record (i.e., tied to the thaw operation it describes).", "This model is a specialized kind of Result used specifically for cryobag thaw events and follows the common result lifecycle and semantics."]}
{"file": "app/models/therapy_image.rb", "type": "model", "domain_rules": ["A TherapyImage represents an image from a therapy session that is typically produced by an Equipment run.", "Each Equipment run generates a Datum and a TherapyImage; they are paired by epoch time (Datum.therapy_epoch == TherapyImage.start_epoch).", "The Datum association is scoped to the same equipment (the link is only valid when the TherapyImage and Datum share the same equipment_id).", "Association to Equipment and Datum is optional (a TherapyImage may exist without an associated Equipment or Datum), but when present they should correspond to the same equipment run/epoch.", "The image is uploaded in base64 form and must pass integrity, processing, and download validations before it is accepted.", "TherapyImage maintains image versions (derivatives) and will recreate those versions when crop parameters (e.g., crop_x) are provided, enabling cropping workflows.", "Equipment sends Datum and the associated TherapyImage separately and asynchronously, so the system must tolerate out-of-order arrival and link them by epoch and equipment_id."]}
{"file": "app/models/storage_unit.rb", "type": "model", "domain_rules": ["A storage unit belongs to an organization and must have a name.", "A storage unit may optionally be linked to a piece of equipment.", "A storage unit holds products and consumables; deleting the storage unit clears their storage reference rather than deleting those items (associations are nullified).", "A storage unit has a container type limited to: Generic, Vials, or Bags.", "If a storage unit uses fixed_capacity, it enforces a grid layout: shelves (1–20), columns (1–10), and rows (1–10); these values must be integers.", "Rows and columns (and shelves when fixed_capacity) are only validated when fixed_capacity is enabled, allowing variable-capacity units otherwise.", "Storage units are implicitly ordered by creation time when listed or exported."]}
{"file": "app/models/task_instruction.rb", "type": "model", "domain_rules": ["A TaskInstruction must have a reference value (reference is required).", "The reference must be unique within the same procedure (no two instructions in one procedure share the same reference).", "Completion is represented by a boolean 'complete' and maps to a localized status text: when complete is true it uses I18n.t('attributes.complete'), otherwise I18n.t('incomplete').", "The TaskInstruction's outcome combines the localized completion status with the base Task outcome (parent outcome) and the final string is sanitized for safety.", "TaskInstruction extends Task behavior — it augments the parent task's outcome rather than replacing it."]}
{"file": "app/models/result_akta_ready450_hda.rb", "type": "model", "domain_rules": ["This model stores time-series chromatogram data imported from Akta Ready 450 HDA ZIP packages that must contain a 'Result.xml' and referenced chromatogram .Xml and binary CurvePoints files.", "item_value must be numeric, and there can be only one ResultAktaReady450Hda per combination of datum_id, item_name and logged_at (uniqueness constraint).", "Curve names from the source are normalized to canonical item_name values via a mapping (unmapped names are kept as-is) before storing results.", "Timepoints are reconstructed from curve metadata: item_hour = dist_to_start_point/60 + (dist_between_points/60 * index); logged_at = ChromatogramStartTime minus ChromatogramStartTimeUtcOffsetMinutes plus item_hour hours.", "Binary CoordinateData files are parsed (skipping header bytes) into float arrays of item_values and item_volumes; entries with blank value or blank volume are ignored.", "Volume records are created only for unique (value, volume) pairs per curve (duplicate pairs are deduplicated).", "EventCurves yield run_log and fraction records; fractions are stored separately and the datum's extended_header.run_log is updated with run_log.", "Import updates the import datum metadata: name and operator_instrument from Result.xml, start_time and end_time are set based on the earliest and latest logged_at of imported results (initially start_time may be set to current time).", "If the uploaded file is not a ZIP or Result.xml is not found, the import marks the datum as finished and an unsupported-file event is emitted.", "After successful import the first and last result for the datum trigger downstream updates: the first result updates datum-level result metadata, and the last result updates equipment last-result metadata and datum-level result metadata.", "All result, volume and fraction records for a single import are persisted together (atomic import) to ensure consistency."]}
{"file": "app/models/clinic.rb", "type": "model", "domain_rules": ["A clinic must have a name, address, zip, country, te_code, and facility_number.", "te_code is a 6-character identifier and must be unique within the same organization.", "facility_number is a 5-character identifier and must be unique within the same organization.", "website, if provided, must be a valid URL.", "A clinic may optionally belong to an organization and may optionally be assigned to a folder (parent).", "Clinics have many products and documents; deleting a clinic does not delete those records but disassociates them (their clinic FK is nullified).", "Clinics are versioned (audit trail of changes is kept).", "Search for clinics (respecting user policy scope) matches name, te_code, or facility_number with case-insensitive partial matching; when no search term is given the results are limited (default 5) to the user-visible scope.", "CSV export of clinics provides name, website, contact, email, and phone, ordered by clinic name.", "The clinic's full_name is the clinic name and can be included in JSON representations when requested (controlled by serialization options)."]}
{"file": "app/models/event.rb", "type": "model", "domain_rules": ["Every event must belong to an organization.", "Events have a typed severity: unknown (default), information, operator_acknowledgement, warning, or error; unrecognized assigned values are recorded in original_unknown_event_type.", "On create, an event must have a date in most cases; the date requirement is skipped only when the associated equipment (direct or via datum) is of type sepax2 or sefia.", "Events may be associated with a datum, equipment, gateway, sensor, or a parent folder (optional associations); equipment/datum association influences timezone, facility notifications, and webhook behavior.", "Acknowledgements, notes, and tags are attached to events (polymorphic) and are removed when the event is deleted.", "An event is considered acknowledged if it has any acknowledgements; operator_acknowledgement is treated similarly to an acknowledged state.", "After creating an event, if the event is tied to equipment or a datum whose equipment has a facility, the facility dashboard is notified of the new operation.", "After saving an event, the parent datum’s status is recalculated: acknowledged events set datum to HAS_ERRORS if datum.errors? else HAS_WARNINGS if datum.warnings? else HAS_ACKNOWLEDGEMENTS; warning events set datum to HAS_ERRORS if datum.errors? else HAS_WARNINGS; error events set datum to HAS_ERRORS.", "After saving a warning or error event that is associated with a datum, alarm_created webhooks for the event’s organization are enqueued for delivery.", "Event comments are sanitized before validation to remove embedded JavaScript.", "Events can be marked privileged; queries and search support filtering by privileged vs not_privileged, by folder (parent_id), and by tags (must include specified tags).", "CSV export of events includes: comment, alarm type and value, event type, timestamp, equipment name, and datum name; timezone for searches is taken from datum.equipment or gateway, falling back to UTC."]}
{"file": "app/models/segment.rb", "type": "model", "domain_rules": ["A Segment belongs to a freezing profile (freezing_profile_id) and segments are ordered by a non-negative integer sequence; the lowest non-negative sequence is the first segment.", "sequence is required and must be an integer >= 0; segments are implicitly ordered by sequence ascending.", "Segment types are drawn from: ramp, dwell, heat (human-readable translation provided).", "Custom validation (SegmentValidator) enforces additional domain constraints on a segment (not shown here).", "For 'ramp' and 'heat' segments the segment end temperature is the ramp_target; for 'dwell' it is the previous segment's end temperature unless it's the first segment, which defaults to 25.0. Unknown types default to 25.0.", "The previous temperature for calculations defaults to 25.0 for the first segment; prev_temp returns nil if the profile or sequence prerequisites are not met or sequences are blank.", "Duration for 'ramp' and 'heat' segments is computed from the temperature difference to the target divided by the ramp_rate (defaults: target 0.0, rate 1.0) and converted to minutes; first segment uses 25.0 as the starting temp.", "Dwell segment duration is derived from dwell_time (HH:MM) converted to seconds, but only when dwell_time parses successfully and is <= 24 hours; otherwise duration is zero.", "End time is cumulative: first segment end_time = its duration; subsequent segments end_time = previous_segment.end_time + duration.", "Segments are uniquely identifiable by an MD5 hash of their configuration (segment_type, ramp_target, ramp_rate, dwell_time, lid_alarm_on, dwell_at_end).", "dwell_time is stored as HH:MM and parsed into seconds; invalid or non-dwell values yield nil for parsed seconds.", "Ramp/heat ramp_rate bounds: ramp max 2.0, heat max 1.5; ramp and heat minimum ramp_rate is 0.01; other types have 0 allowed. The ramp adjustment step is 0.01.", "Ramp/heat ramp_target bounds: minimum -100 and maximum 40; other types have target bounds of 0.", "Helper predicates exist to test type: dwell?, ramp?, heat?.", "Calculations avoid database re-querying so logic relies on the profile's in-memory segments (important for nested creation flows)."]}
{"file": "app/models/result_xuri_hda.rb", "type": "model", "domain_rules": ["Represents timestamped HDA measurement results from Xuri; each record captures item_name, item_hour, item_value and logged_at.", "Records are implicitly ordered by logged_at (logged_at is the default ordering column).", "CSV export is limited to the fields: item_name, logged_at, item_hour, and item_value; no field renaming/mapping is defined.", "item_value must be numeric.", "A measurement's item_value must be unique within the combination of datum_id, item_name, and logged_at (prevents duplicate measurement records for the same datum/item/timestamp/value).", "Model includes shared measurement behavior from DatumResults (common domain logic for datum results is applied)."]}
{"file": "app/models/result_akta_readyflux_hda.rb", "type": "model", "domain_rules": ["This model stores chromatogram result points for Akta Readyflux HDA and is ordered by logged_at.", "Each result belongs to a datum (datum_id) and is identified by item_name and timestamp: item_value must be numeric and unique per (datum_id, item_name, logged_at).", "Only these CSV fields are accepted for import: item_name, logged_at, item_hour, item_value.", "File import accepts only ZIP archives; non-zip or unsupported zip contents mark the import_datum as ended and emit an unsupported-file event.", "A valid ZIP must include Result.xml; Result.xml supplies import metadata (name, operator_instrument) and Created time which is adjusted by CreatedUtcOffsetMinutes to compute start_time.", "The chromatogram XML referenced in Result.xml is parsed to extract curves and event run_log; run_log is stored in datum.extended_header.", "Curve amplitude data are read from a referenced binary file, decoded into floating-point item values and persisted as individual result rows.", "Each result's item_hour is computed from DistanceToStartPoint and DistanceBetweenPoints (converted to hours), and logged_at = curve start_time + item_hour hours; ChromatogramStartTime is adjusted by ChromatogramStartTimeUtcOffsetMinutes to get UTC start_time.", "Curve names from the instrument are normalized to standardized item_name values via a mapping (e.g., '%Flux_Drop' → 'percent_flux_drop', 'AE121pH' → 'ph', 'AE131UV' → 'uv', etc.).", "All curve points for a datum are bulk-imported inside a database transaction; if import fails, changes are rolled back and datum is not partially updated.", "After import completes, import_datum.end_time is set to the logged_at of the last persisted result (or current time if Result.xml missing), and the first/last results are used to update aggregated datum and equipment state (update_result_in_datum, update_last_result_in_equipment).", "If Result.xml is absent in a ZIP, the import marks the datum ended immediately and triggers an unsupported-file event instead of importing results."]}
{"file": "app/models/result_akta_ready_volume_hda.rb", "type": "model", "domain_rules": ["Records represent AKTA HDA 'ready volume' measurement results tied to a datum (via datum_id).", "item_value must be numeric (i.e., a valid number).", "A given numeric item_value must be unique within the combination of datum_id, item_name, and item_volume (no duplicate values for the same datum/item name/volume).", "Only these fields are exported to CSV: item_volume, item_name, and item_value.", "There is no CSV field remapping defined (CSV field names are used as-is)."]}
{"file": "app/models/task_start_freeze.rb", "type": "model", "domain_rules": ["This task represents initiating a freeze step within a procedure.", "Each TaskStartFreeze must have a reference value and that reference must be unique within the same procedure.", "The task has a completion state that is reported as a localized 'complete' or 'incomplete' label combined with the superclass outcome.", "The task exposes no other associated domain object (associated_object is nil).", "A TaskStartFreeze is considered ready only if there is a FreezerLink for the same procedure and that FreezerLink reports all of its items as ready."]}
{"file": "app/models/result_sefia.rb", "type": "model", "domain_rules": ["Represents measurement results produced by a Sefia instrument (each record is an instrument datum).", "Results are implicitly ordered by the 'time' timestamp field (default ordering key).", "CSV export is supported but restricted to a curated whitelist of fields — only the listed sensor, status and metadata attributes may be exported.", "The whitelist includes timestamp/identity fields (logged_at, time, chrono), many sensor/channel readings (e.g. t01–t12, cuvv1–cuvv6, volt*, dac_pelt_*, abd*, pr*, fan*, pelt*, mixing*, pump, weight1–4, state_id, etc.), isolation module sensors, control/status bits, and a comment field for notes.", "No CSV field remapping is defined (the CSV fields map is empty), so exported column names are the same as the whitelisted attribute names.", "Model behavior is extended by the DatumResults module, indicating shared/result-specific behaviors or validations are applied from that concern."]}
{"file": "app/models/task_multiple_questions.rb", "type": "model", "domain_rules": ["A TaskMultipleQuestions must have a reference and that reference is unique within the same procedure.", "The task models multiple questions: its stored value is expected to be a JSON array of answers aligned by index to a questions array.", "If the stored value is blank or cannot be parsed as JSON, the task’s outcome is treated as an empty array (no answers).", "Single quotes are removed from the stored value prior to JSON parsing (inputs may include single quotes).", "Each outcome entry contains question text from question['detail'] (or empty string if missing) and a formatted answer.", "For questions with question_type == 'Number', the question's question_options are appended as a suffix to the answer (e.g., units) via a concat routine.", "The final outcome is a sanitized JSON array of { question, answer } objects concatenated with the parent Task outcome.", "JSON parsing errors are handled gracefully by returning an empty outcome rather than raising exceptions."]}
{"file": "app/models/facility_user.rb", "type": "model", "domain_rules": ["Each FacilityUser record associates a single user with a single facility.", "Every FacilityUser must belong to a facility.", "Every FacilityUser must belong to a user.", "Serves as the join model representing user-to-facility membership (users can be linked to facilities and facilities can have multiple users)."]}
{"file": "app/models/sso_provider.rb", "type": "model", "domain_rules": ["An SSO provider has a type (saml or oauth2).", "SAML providers must provide an issuer and domain_names (one or more semicolon‑separated domains).", "For Pre‑GMP deployments, SAML domain_names may not include common public email domains: gmail.com, googlemail.com, hotmail.com, hotmail.co.uk, yahoo.com, outlook.com, microsoft.com, live.se.", "OAuth2 providers must provide both oauth_client_id and oauth_client_secret.", "A provider name is required when the provider has a type or is marked active, and the name must pass the sso_provider_name validation.", "An SSO provider may belong to an organization, but that association is optional.", "Only providers marked active are used to resolve a user’s SSO provider from an email address; the domain part of the email (lowercased) is matched against providers' semicolon‑separated domain lists.", "Domain‑based lookup returns the first active provider whose domain list includes the email domain.", "Role mappings are stored as JSON (roles_matching) and are parsed into a hash; parsing errors are logged and result in an empty mapping."]}
{"file": "app/models/task_q_r_code_equip.rb", "type": "model", "domain_rules": ["This task represents scanning an equipment QR code and may be linked to an Equipment record (stored in the task's value field); the equipment association is optional.", "QR code input is validated by a specialized QrCodeEquipmentValidator which enforces QR/equipment-specific business rules before the task is valid.", "Each task must have a reference and that reference must be unique within the same procedure (no two tasks in the same procedure can share the same reference).", "When the task is part_of_equipment_group, saving the task attempts to link the referenced equipment to the procedure's freezer link (but only if the task has a non-blank value and the equipment is visible to the current user).", "The freezer-linking operation uses the freezing_protocol_id taken from the procedure task identified by reference_to (it expects that task to contain a profile_id).", "If this task is the final QR-code scanning task for the equipment group (i.e., it has the highest sequence among TaskQRCodeEquip tasks with part_of_equipment_group true in the same procedure), a finished-scanning notification is sent for the procedure's freezer link.", "Determination of final group scan: compare this task's sequence to the maximum sequence of TaskQRCodeEquip tasks with part_of_equipment_group true within the same procedure.", "When resolving the associated equipment object, prefer the next reified version if an associated_object_version_id is present and the next version exists; otherwise return the base equipment. If the referenced version is missing, raise a RecordVersionNotFound domain error.", "Outcome display includes an edit link to the associated equipment (if present) followed by the task's usual outcome content; the combined output is sanitized for safe HTML rendering.", "Freezer linkage will not proceed if the equipment cannot be found in the policy scope for the current user (i.e., users can only link equipment they are authorized to access)."]}
{"file": "app/models/equipment_profile.rb", "type": "model", "domain_rules": ["An EquipmentProfile represents a link between one Profile and one Equipment; each record must reference a specific profile and a specific equipment.", "Within a single Profile, the same Equipment cannot be associated more than once (Equipment is unique per Profile).", "This model acts as the join/association entity connecting profiles and their equipment assignments."]}
{"file": "app/models/product.rb", "type": "model", "domain_rules": ["Every product belongs to an organization and may optionally belong to a user, project, facility, clinic, batch, parent product, storage vessel, storage unit, and folder.", "Products can form parent/child hierarchies; deleting a parent destroys its child products and a product cannot be its own parent or have a parent from a different organization.", "A product must have a name and a product_code. product_code must be exactly 8 characters unless the product is an mRNA, created from a therapy image, or skip_validate_product_code is set.", "Root products (no parent_product) may have a donation_ref that must be 13 characters and unique within the same clinic, parent_product scope and organization.", "Quantity is required. Unit defaults to 'mL' for new records and quantity_with_unit returns '-' if quantity is missing.", "division_number is required, defaults to '01' if absent, and must be exactly two alphanumeric characters.", "critical_temperature must be numeric between -196 and 40 (inclusive).", "On create, blood_group and expiration_date are required unless the product is mRNA or created from a therapy image.", "FacilityProjectMemberValidator must pass (product’s project and facility membership must be valid for the organization).", "When storage_vessel is unset, the product’s storage_unit and location are cleared; storage tracking updates occur whenever storage_unit_id changes.", "Storage changes increment or decrement storage unit item counts and recalculate free space; putting into storage sets stored_at and increments storage_count, removing accumulates storage_duration and clears stored_at.", "Products track storage_duration and live_storage_duration includes elapsed time since stored_at when present.", "Status values are constrained to predefined enums (quarantined, in_stock, checked_out, consumed, rejected, recalled, lost_destroyed); expired is true only for in_stock or checked_out when expiration_date has passed.", "A product is considered missing if its status is in_stock but no storage_unit is assigned.", "Documents are deleted when a product is deleted; associated data records are nullified; product reservations are deleted with the product.", "Components are stored as an array with typed entries (cells, liquids, solids) and must include name, quantity and unit. Components setters enforce type-specific updates and maintain sequence order; free-form components_list= setter is disabled.", "A fixed set of storage condition labels exists (e.g. '37°C', 'Ambient', '2-8°C', '-20°C', '-80°C', 'Below -120°C', 'Below -150°C'), and storage_conditions is among fields exposed for formulas and read/write operations.", "Selected product attributes are whitelisted for formula read/write operations (custom fields, components, quantities, storage_conditions, critical_temperature, etc.).", "Before creation, project_id and facility_id may be auto-associated from the parent product or from the current user's first associated project/facility when the current user has approver/operator roles and those IDs are not already set.", "User associations are validated for privacy and organization consistency when changed; project and user must belong to the same organization as the product.", "product_with_division formats product_code and division_number as a combined identifier (division number padded to three digits)."]}
{"file": "app/models/ceres_profile.rb", "type": "model", "domain_rules": ["CeresProfile is a Ceres-specific Profile grouping that owns many profiles_variables; those variables are deleted when the profile is deleted.", "profiles_variables must be valid (their validation errors bubble up) and can be managed via nested attributes on the profile.", "A profile must have a group_name (required) and it cannot exceed 80 characters.", "group_name is normalized to uppercase before validation, so uniqueness is effectively enforced on the uppercase form.", "group_name must be unique within the combination of organization, live_version, and protocol (no duplicate group names per organization/live_version/protocol).", "Validations can be deferred/delayed (supports delayed validation workflows).", "The approver for a profile is the user who created the most recent version; if there are no versions, approver is nil (profile changes are versioned/audited).", "The model exposes an MD5 hex representation and a compact cable-ready object (id, name, class, group_name, protocol name) for API/real-time compatibility."]}
{"file": "app/models/package.rb", "type": "model", "domain_rules": ["Package represents a versioned software package/resource (a software version) used by the Todos v2 API.", "A package is identified by a SHA1 hash that represents the package version/content.", "A package includes a human-readable label as metadata alongside its SHA1.", "Clients and consumers of this resource use the Todos v2 media type (Accept: application/vnd.todos.v2+json).", "Package inherits behavior from SoftwareVersionBase, so standard software-versioning semantics apply to it."]}
{"file": "app/models/training_course.rb", "type": "model", "domain_rules": ["A training course must belong to an organization.", "A training course may be associated with a creating user and an approval team, but those are optional.", "A training course must have a name and description.", "A training course must specify a validity period and a duration; both are required and must be valid duration strings.", "Validity period and duration are interpreted as time intervals and can be converted to seconds for scheduling/expiry logic.", "A training course has versioning/audit history (changes are tracked).", "Training records associated with a course are deleted when the course is deleted.", "Documents attached to a course are deleted when the course is deleted.", "Associations to users and procedures are cleared (nullified) rather than deleting those records when the course is deleted.", "A course has an approval state: if not approved, it requires approval (requires_approval? == !approved).", "A course can be marked archived; archived courses have their displayed name prefixed with a localized 'archived' label.", "Course searches are scoped by user permissions and support case-insensitive name matching; when no search term is provided results are limited to a default maximum and still scoped by policy."]}
{"file": "app/models/task_q_r_code.rb", "type": "model", "domain_rules": ["A TaskQRCode may reference a Product (the product identifier is stored on the task); the association is optional so a task can exist without a linked product.", "The QR code content must pass domain validation (QrCodeProductValidator) — QR codes must represent a valid product (and optional version) according to application rules.", "Each TaskQRCode must have a reference value, and that reference must be unique within the scope of the same procedure.", "If an associated_object_version_id is provided, the task resolves to the specified product version's next version (reified) when available; if no version id is provided the current product is used.", "If a specified product version cannot be found, the task raises a domain-level RecordVersionNotFound error to indicate an invalid target version.", "The task outcome includes a sanitized HTML edit link to the associated product (when present), prepended to the task's inherited outcome content."]}
{"file": "app/models/result_chromatogram.rb", "type": "model", "domain_rules": ["This model represents chromatogram result points (datum_id, curve_number, volume, amplitude, timestamps) imported from instrument files.", "Only ZIP (Unicorn) and NetCDF (.cdf) result files are supported for import; other extensions are rejected and the datum is marked ended.", "ZIP imports require a Result.xml at the root; Result.xml provides the result name, created timestamp with UTC offset (sets datum start_time/end_time), operator, and column metadata used to populate datum.comment.", "ZIP chromatograms must include a chromatogram XML (named by ChromatogramFileName) and referenced binary CurvePoints files containing CoordinateData.Volumes and CoordinateData.Amplitudes; each paired volume+amplitude becomes a ResultChromatogram record.", "NetCDF imports require specific variables/attributes (ordinate_values, actual_sampling_interval, actual_run_time_length and attributes like sample_name, injection_date_time_stamp, detector_name); these populate datum metadata and produce point records.", "Each created chromatogram point must have both a non-blank volume and amplitude; only such pairs are persisted.", "Volume and amplitude are treated as floating-point measurements; curve_number is an integer (NetCDF imports set curve_number = 1).", "Run Log events found in the chromatogram XML are converted into Event records of type 'information' attached to the datum, with event times = datum.start_time + EventTime (minutes) and the datum's organization_id.", "Datum start_time is derived from file metadata (with UTC offset applied); for ZIP the end_time is initially set equal to start_time, for NetCDF end_time = start_time + actual_run_time_length.", "Datum.comment includes column name and injected volume information when available from the file metadata.", "Imports create many ResultChromatogram records in bulk (batched), but only when valid point data exist; failures during parsing/logging return nil and are logged.", "After successful import, the most recently created ResultChromatogram is touched to trigger DatumResults callbacks and webhooks are delivered (indicating results_skipped: true).", "CSV interactions are limited to whitelisted fields: logged_at, curve_number, volume, amplitude."]}
{"file": "app/models/task_question.rb", "type": "model", "domain_rules": ["Each TaskQuestion must have a reference and that reference is unique within its associated procedure (no two questions in the same procedure share the same reference).", "A TaskQuestion can be marked as numeric via the is_numeric attribute.", "When generating an outcome, the question uses its value (or an empty string if value is nil) concatenated with the Task superclass output.", "All outcome text is sanitized before being returned, ensuring question output is safe for presentation.", "TaskQuestion is a specialized Task and thus participates in Task-level behavior and workflows."]}
{"file": "app/models/gateway.rb", "type": "model", "domain_rules": ["Gateway is a specialized Equipment representing a network gateway device.", "Gateways have many accessories, but only 'active' accessories are considered; accessories are removed if the gateway is deleted.", "Gateways have many events which are deleted when the gateway is removed.", "Gateways can be linked to multiple other equipment via equipment_gateway associations; those join records are removed when the gateway is deleted.", "Gateways support nested creation/update of their equipment associations (equipment_gateways).", "Each gateway has a device_type constrained to the configured gateway device-type enumeration.", "On creation, a gateway must have a hostname, and that hostname must be unique within the same organization among active gateways."]}
{"file": "app/models/task_webhook.rb", "type": "model", "domain_rules": ["A TaskWebhook must have a webhook_uri and, outside the development environment, that URI must use HTTPS (starts with https://).", "Outside the development environment, webhook_uri is validated by PublicIpValidator (e.g., to prevent disallowed/internal IP addresses or ranges).", "A TaskWebhook may be associated with an endpoint: the value field is used as an id reference and can point to either a WebhookDelivery or a Webhook record.", "When resolving the associated object, webhook deliveries take precedence over webhooks (associated_object returns webhook_delivery if present, otherwise webhook).", "Authorization header values for webhooks are stored encrypted (signed) in webhook_encrypted_headers and are only included/stored when an authorization value is provided.", "Decrypted webhook headers are returned as a symbolized hash; if decryption fails or there are no encrypted headers, an empty hash/result is returned.", "Assigning a new value clears the cached associated_object so subsequent lookups reflect the updated referenced webhook or delivery."]}
{"file": "app/models/process_procedure.rb", "type": "model", "domain_rules": ["A ProcessProcedure is an ordered eSOP step that belongs to a Procedure and therefore to that Procedure's Organization.", "Each step has a sequence number that determines its order within the procedure; human-facing labels show sequence + 1 (e.g., 'eSOP 1').", "ProcessProcedure records are ordered ascending by sequence when listed.", "A ProcessProcedure may optionally be associated with a manufacturing process (mfg_process) and with a Team; changing the record will update the associated mfg_process timestamp.", "Business validation (MoveMfgProcessForwardValidator) enforces rules around assigning or advancing the associated manufacturing process (i.e., mfg_process can only be moved forward according to domain rules).", "Starting work for a batch uses the ProcessProcedure's Team and sequence: starting invokes the Procedure-level start_by_team with the team, batch id, batch_sequence = this step's sequence, and any start_after_with_task instructions.", "start_after_with_task encodes follow-on start rules: it contains entries with 'sequence' (used as integers) and optional task references; these can be converted to a human string of 1-based sequence numbers with optional task refs (e.g., '3:TASKREF').", "Available sequences is a transient (non-persisted) attribute used at runtime (e.g., for UI/selection) and does not persist business state.", "Changes to ProcessProcedure are audited (versioned) except for updated_at metadata."]}
{"file": "app/models/result_akta_ready450_fraction_hda.rb", "type": "model", "domain_rules": ["Records represent timestamped volume measurements and accompanying messages produced by an ÄKTA Ready 450 fractionation run.", "A measurement's volume must be a numeric value.", "Duplicate measurements are prevented: there must not be more than one record with the same datum context, timestamp (logged_at), message and volume.", "Only the fields logged_at, volume and message are intended for CSV export/import; there is no CSV field remapping configured.", "The model uses shared DatumResults behavior, so each measurement is tied to the broader datum/data-context managed by that concern."]}
{"file": "app/models/task_record_service.rb", "type": "model", "domain_rules": ["A TaskRecordService is tied to a Service record (the task may optionally reference a service).", "Each TaskRecordService must have a reference value, and that reference must be unique within the same procedure.", "When an associated service version ID is provided, the task resolves to the next reified version of that service if one exists; otherwise it falls back to the current service.", "If a specified associated service version cannot be found, a RecordVersionNotFound exception is raised.", "Task outcomes include an edit link to the associated service (if present) and the final outcome content is sanitized before use."]}
{"file": "app/models/result_akta_flux.rb", "type": "model", "domain_rules": ["Only CSV files are accepted for AKTA flux imports; any other extension aborts import, sets the import end_time to now and creates an error Event ('unsupported_file_type').", "The importer ignores the first 10 rows of the file and begins reading data at row 11; the timestamp in the first data row becomes the import start_time and the timestamp of the last data row becomes end_time.", "The first column of each data row is a timestamp (parsed to UTC) used as logged_at for all measurements, warnings, and alarms recorded for that row.", "Only a predefined set of columns are imported and mapped to fixed item names (flux headers) — measurement columns become ResultAktaFlux records with datum_id, item_name, item_value and logged_at.", "Measurement cells that are nil are skipped (no record created).", "Columns 30 and 31 are special: column 30 contains warnings and column 31 contains alarms; entries equal to 'NONE' are ignored, otherwise they are collected into import_datum.extended_header as {date, message} entries.", "After a successful import the import_datum is updated with the source filename, start_time, end_time and the collected extended_header (warnings/alarms); the import method indicates success by returning whether any ResultAktaFlux records were created.", "Bulk insertion is used to create result records; after import the first and last result updated_at timestamps are refreshed to trigger downstream callbacks that populate related datum and equipment metadata.", "CSV field set expected/whitelisted for processing includes: item_name, logged_at, item_hour, and item_value."]}
{"file": "app/models/task_create_product.rb", "type": "model", "domain_rules": ["A TaskCreateProduct may optionally reference a Product using the task's 'value' field as the product foreign key; if no product is linked, associated_object returns nil.", "Each task must have a reference value, and that reference must be unique within the same procedure (reference scoped uniqueness).", "Product creation/assignment behavior for this task is governed by a custom validator (CreateProductValidator) which enforces additional domain rules for creating or configuring a product.", "The task can target a specific product version (associated_object_version_id). When resolving: it looks up the specified version; if that version has a next version it reifies and returns that next version instance, otherwise it falls back to the current product; if the requested version cannot be found it raises Exceptions::RecordVersionNotFound.", "The task outcome is returned as sanitized HTML; when an associated product exists the outcome is prefixed with a link to the product's summary that navigates to the product edit page, followed by the default outcome content."]}
{"file": "app/models/instrument.rb", "type": "model", "domain_rules": ["Instrument is a specialized Equipment (device_type set to instrument).", "An instrument can have many accessories (only active ones are considered) and deleting an instrument removes its accessories.", "Instruments have configurable clean_hold_time and dirty_hold_time values; both must be valid duration strings.", "When cleaning_status changes to 'clean' and a clean_hold_time is present, the system schedules cleaning_status_will_change_to_dirty_at = now + clean_hold_time.", "When cleaning_status changes to 'dirty' and a dirty_hold_time is present, the system schedules cleaning_status_will_change_to_expired_at = now + dirty_hold_time.", "The model tracks next-transition timestamps for cleaning status and exposes human-readable countdowns based on the current status and those timestamps.", "Hold time strings are parsed into seconds to compute scheduled transition times.", "Scheduling of the next cleaning-status transition occurs immediately after a saved change to cleaning_status.", "Importing data for an instrument is performed in the UTC time zone and executed on behalf of the specified user."]}
{"file": "app/models/io_param.rb", "type": "model", "domain_rules": ["An IOParam belongs to a Datum.", "An IOParam must have date, io_type, and label present.", "io_type is an enum with named values: unknown (-1), setpoint (0), result (1), process_info (2), threshold (3), and warning (4); default is unknown.", "The model preserves raw integer io_type values from the database (to avoid losing unknown integers) and treats non-nil raw io_type only if it is an Integer; non-integer raw io_type values are invalid.", "The io_type getter returns the enum name, but if the stored value is an unknown integer it returns 'unknown' to avoid casting to nil.", "Duplicate IOParams are defined by the tuple (datum_id, date, io_type, label, value); helper methods exist to detect duplicates in-memory and in the database using that slice.", "There is no automatic uniqueness DB constraint in the model code shown; uniqueness must be enforced via the provided helper methods or elsewhere.", "value_and_unit formats the numeric value and its unit as a single string (\"value unit\") and returns an empty string when value is absent.", "Records are implicitly ordered by created_at unless overridden."]}
{"file": "app/models/batch_schedule.rb", "type": "model", "domain_rules": ["A batch schedule belongs to an organization.", "A batch schedule must have a start time (started_at), an end time (ended_at), and a description.", "The start time must be before the end time; otherwise the record is invalid.", "Batch schedules aggregate four reservation types (equipment, user, consumable, product) and reflect their time span.", "The schedule's started_at is set to the earliest associated reservation start and ended_at to the latest associated reservation end (can exclude a single reservation when recalculating).", "Schedules are considered overlapping when started_at <= other_end AND ended_at > other_start (used to find conflicting schedules).", "A schedule is 'pending' if its started_at is in the future (started_at > current time in UTC).", "A schedule cannot be deleted once it has started or is in progress; deletion is only permitted while pending (attempts to delete otherwise are blocked).", "When a batch schedule is deleted (only allowed if pending), its associated reservations are also deleted.", "Changes to batch schedules are tracked/audited (paper trail/history is maintained)."]}
{"file": "app/models/task_header.rb", "type": "model", "domain_rules": ["TaskHeader is a specialized subtype of Task (inherits Task behaviour) used to represent the header section of a task.", "It is intended to encapsulate metadata and summary information for tasks (distinguishes header/summary data from other task types).", "All validations, associations and business rules defined on Task apply to TaskHeader instances as well."]}
{"file": "app/models/report.rb", "type": "model", "domain_rules": ["Report represents a persisted (database-backed) entity used to generate and store reports.", "Instances of Report are stored in the application's database (i.e., persisted model records).", "This model defines no validations, associations, or callbacks, so no additional domain constraints are enforced at the model layer."]}
{"file": "app/models/setting.rb", "type": "model", "domain_rules": ["Settings are implicitly ordered by creation time (created_at) when retrieved.", "There is a setting named unique_prefix that is an integer counter with a default value of 0.", "When accessing unique_prefix, if a superclass defines and returns a value, that inherited value is used.", "If no superclass value is available (nil/false), the class initializes its unique_prefix to 0.", "unique_prefix is treated as a shared/inheritable counter across the class hierarchy and is persisted as an application setting."]}
{"file": "app/models/result_cellcounter.rb", "type": "model", "domain_rules": ["This model stores cell counter results: live_count, dead_count, total_count and viability, each associated with a datum (datum_id) and a logged_at timestamp.", "Only .cmpp and .csv file formats are supported for importing results; other file types mark the datum as ended and trigger an unsupported-file event.", "For .cmpp and .csv imports the parser looks for fields labeled Live (cells/ml), Dead (cells/ml), Total (cells/ml) and Viability (%) and converts their values to floats for storage.", "The import routine attempts to extract the result date from the filename in YYYYMMDD-#### format; if no date is found the current time is used as logged_at.", "On successful import a ResultCellcounter record is created for the given datum and webhooks are delivered for that new result.", "If parsing the file fails the import returns nil and logs an error; no result is created in that case.", "After importing, the associated datum's start_time and end_time are updated to the minimum and maximum logged_at across that datum's ResultCellcounter records (or set to now if there are none).", "A whitelist of CSV fields is enforced for import/consumption: logged_at, live_count, dead_count, total_count, and viability.", "Results are implicitly ordered by logged_at when retrieving records (logged_at is the implicit order column).", "The import method returns a boolean indicating whether the datum has any associated ResultCellcounter records after processing."]}
{"file": "app/models/freezer.rb", "type": "model", "domain_rules": ["Freezer is responsible for creating and preserving immutable snapshots of Datum objects.", "Snapshots produced by Freezer are immutable (cannot be altered) to ensure consistent state management.", "Freezer is used to maintain historical or consistent views of Datum state across operations."]}
{"file": "app/models/deployment.rb", "type": "model", "domain_rules": ["A Deployment tracks the status of five checks: PostgreSQL (pg), Redis (redis), S3 (s3), Kubernetes (k8s), and a started marker.", "A deployment is considered in progress if any of those five attributes is nil (i.e., a check has not yet been recorded).", "A deployment is considered completed when none of the five attributes are nil (all checks have been recorded, even if some are false).", "A deployment succeeds only if it is completed and all five checks are true/healthy (pg, redis, s3, k8s and started).", "A deployment fails if it is completed but at least one required dependency is false/unhealthy or the start marker is false (i.e., completed but not all checks passed).", "Progress is measured as the count of truthy checks (number of checks that are true) out of a fixed total of 5, with percentage calculated as (completed_count / 5) * 100 and reported as a percent string.", "Only nil indicates an ongoing check; an explicit false indicates a completed check that failed.", "Deployments are implicitly ordered by creation time (created_at), so older deployments are considered earlier in the process."]}
{"file": "app/models/task_upload_document.rb", "type": "model", "domain_rules": ["This model is a Task subtype specifically for handling PDF documents related to tasks.", "Each TaskUploadDocument must have a reference value present.", "The reference must be unique within the scope of the same procedure (no two documents with the same reference under one procedure).", "A PDF file is stored for the task and associated image previews can be stored for that PDF (used for previews).", "If a PDF file is present, the task exposes a download link labeled with the document identifier; no download link is produced when no file is attached.", "The generated download link is presented as HTML, opens in a new tab, includes security attributes (noopener noreferrer), and the link output is sanitized and combined with the parent task outcome."]}
{"file": "app/models/notification_setting.rb", "type": "model", "domain_rules": ["NotificationSetting applies a per-day notification limit to a notifiable entity (polymorphic); the notifiable association is optional so settings can be unassociated or global.", "limit_per_day must be an integer between 0 and 999 inclusive; when unset it behaves as 0 (no allowed notifications).", "notifications_sent represents how many notifications have been sent today and defaults to 0 when unset.", "The model tracks the day via last_notification_date; when last_notification_date is nil or not equal to today, the daily counter is reset to 0 and last_notification_date is set to today and persisted.", "less_than_limit_per_day? enforces the daily limit by performing the daily reset if needed and then returning whether notifications_sent is less than limit_per_day — i.e., whether more notifications may be sent today."]}
{"file": "app/models/workflow.rb", "type": "model", "domain_rules": ["Workflows are ordered chronologically by creation time (created_at) when retrieved by default.", "Each workflow is associated with a subject identified by subject_type and subject_id.", "A subject-workflow is identified by a composite key formatted as \"subject_type:subject_id:workflow_cls\".", "The workflow class (workflow_cls) is part of the identity, so workflows for the same subject but different workflow_cls are treated as distinct."]}
{"file": "app/models/task_profile_specification.rb", "type": "model", "domain_rules": ["This entity is a specialized Task (TaskProfileSpecification).", "It may be linked to a Profile, but the profile association is optional (the task can exist without a profile).", "A reference value is mandatory for each TaskProfileSpecification.", "The reference must be unique within the same Procedure (no two specifications in the same Procedure can share the same reference).", "The model exposes a setter for profile_id to assign/change the linked Profile."]}
{"file": "app/models/result_monitor.rb", "type": "model", "domain_rules": ["Only CSV files are accepted for bulk import; non-CSV uploads are ignored and the import is marked completed (end_time set).", "Imported CSVs must contain only whitelisted measurement fields (logged_at, co2, temp, humidity and channel_1..channel_6 particle counts).", "Each non-header CSV row becomes a ResultMonitor record associated with the originating datum (imported_datum), its equipment, and its organization.", "After import, the importing datum's start_time and end_time are set to the minimum and maximum logged_at values of its ResultMonitor records (or to now if none exist).", "Readings are assigned to daily logs according to the equipment's time zone; a data operation is used to locate the correct daily log for each reading and rows with no matching daily log are skipped.", "When a reading is appended to a daily log, it is stored with that daily log as its datum and the daily log's latest reading triggers webhook delivery (notifying that results were added, with results_skipped flag).", "A reading will update the equipment's last_resultable only if it has a logged_at and is newer than the equipment's current last_resultable; before updating, an error-status determination operation is run for the new reading.", "Readings lacking an associated equipment or logged_at are not used to update equipment last_resultable.", "CSV imports use the allowed fields as-is (no field mapping is applied), and creation failures during import are logged and do not halt processing of the file."]}
{"file": "app/models/facility.rb", "type": "model", "domain_rules": ["A facility belongs to an organization and its full display name is the organization name followed by the facility name.", "Facility name is required and must be unique within the same organization.", "Location is required; when the location changes the facility is geocoded to produce latitude/longitude.", "Facilities may have a floorplan image/document attached.", "A facility can be assigned a reviewer team and an escalation team for review/escalation workflows.", "A facility may be placed in an optional parent folder.", "Facilities have many users via facility-user assignments; those assignments are removed if the facility is deleted.", "Facilities own batches, consumables, data records, documents, procedures, products, and gateways — these are deleted when the facility is deleted.", "Equipment associated with a facility is scoped to active equipment only and ordered by creation date; when a facility is deleted equipment is disassociated (facility reference nulled) rather than deleted.", "Equipment can be created/updated via nested attributes on the facility (managed through the facility form/API).", "Facilities are audited (versioned) and can be exported to CSV listing name, description and location in creation order.", "Search and listing of facilities are access-controlled (policy scope) and support name text search with a default limited listing."]}
{"file": "app/models/task_q_r_code_consum.rb", "type": "model", "domain_rules": ["A TaskQRCodeConsum optionally references a Consumable (the task's 'value' field stores the consumable id) and may also reference a ConsumableMaster; either association can be absent.", "Reference is required for each task and must be unique within the same procedure (no two tasks in one procedure can share the same reference).", "QR-code-specific validation rules are enforced via a custom QrCodeConsumableValidator (consumable-related business constraints are validated before saving).", "The task can be tied to a specific historical version of a consumable: when an associated_object_version_id is present, the task resolves that version (and if that version has a next version, the next version is reified and used).", "If the referenced consumable version cannot be found, the system raises a RecordVersionNotFound error (the task must reference an existing consumable/version).", "When no version id is provided but a consumable is associated, the current consumable is used as the associated object.", "Task outcomes include a sanitized HTML edit link to the associated consumable when present, indicating the task should surface an editable reference to the related consumable."]}
{"file": "app/models/mfg_process.rb", "type": "model", "domain_rules": ["A manufacturing process must belong to an organization and must have a name.", "Processes can be templates; template processes require approval (requires_approval? true when template && !approved).", "Templates get automated document numbering: when creating a template, assign the next document_number as max(organization.last_process_document_number, existing max) + 1 and apply the organization’s document_prefix (with '<TYPE>' replaced by 'PCS').", "When a template’s document_number exceeds the organization’s last_process_document_number, update the organization’s last_process_document_number to that value.", "The public document identifier is document_prefix concatenated with a zero-padded document_number (returned as document_prefix_and_number and included by default in queries/exports).", "Processes have many ordered process procedures; starting a process initializes each procedure in sequence but skips procedures that are waiting for prerequisite tasks or procedures.", "A next procedure is started only when its prerequisites are satisfied: either a direct go_to_sequence trigger matches and no task is involved, or procedure_ready? confirms all start_after sequences are finalized (or required specific tasks are completed).", "A procedure’s start-after entry must reference a sequence that is non-negative and strictly less than the procedure’s own sequence; if a task is referenced the task ref must be present (validated before save).", "Do not start duplicate procedures for a batch: if a batch already has a procedure with the same batch_sequence, the process will skip starting it.", "When new procedures are initiated for a batch, the batch is broadcasted (BatchChannel) to notify listeners.", "Introduction and product_description fields are sanitized to remove embedded JavaScript before validation (preventing stored script content).", "Processes own batches, process_procedures and reviews; removing a process will delete dependent batches, procedures and reviews, while change notifications are retained but nullified.", "User association is optional but when user_id changes it must pass a user privacy policy validation.", "Processes are versioned/audited (paper_trail) and expose CSV export fields including name, description, user, document identifier, version and approver."]}
{"file": "app/models/sefia_profile.rb", "type": "model", "domain_rules": ["Every SefiaProfile must have a group_name; it is required, automatically converted to uppercase before validation, and limited to 80 characters.", "group_name must be unique within the combination of organization, live_version, and protocol (no duplicate group names for the same organization + live version + protocol).", "A SefiaProfile is associated with a protocol and exposes protocol metadata (name, num, ver) for real‑time updates/serialization.", "A SefiaProfile manages many profiles_variables which can be created/updated via nested attributes; those profile variables are destroyed when the SefiaProfile is deleted.", "The ordering of profile variables for display/processing follows the protocol's sorted editable protocol variables (profile variables are matched to the protocol's variable order).", "SefiaProfile is versioned; the approver is derived from the most recent version's whodunnit (returns nil if no versions exist).", "Provides a standardized cable object (id, name, class, group_name, and protocol details) for broadcasting/real‑time use.", "An md5 accessor is provided for API compatibility (exposes an MD5 hex digest interface required by sessions API)."]}
{"file": "app/models/task_loop.rb", "type": "model", "domain_rules": ["TaskLoop is a specialized Task that represents a loop task.", "A TaskLoop must have a reference value (reference is required).", "The reference must be unique within the same procedure (no two TaskLoops in one procedure may share the same reference)."]}
{"file": "app/models/task_timer.rb", "type": "model", "domain_rules": ["TaskTimer is a specialized Task that represents a timer-type task.", "Each TaskTimer must have a reference, and that reference must be unique within the same procedure (uniqueness scoped to procedure).", "Durations are stored on the task as task_duration in seconds.", "You can simulate/set the duration by providing fake_time (hours, minutes, seconds) combined with fake_days; these values are converted into a total number of seconds and assigned to task_duration.", "fake_days is an instance-level simulated day offset that influences the fake_time duration calculation.", "Reading fake_time returns a UTC Time corresponding to task_duration seconds since the Unix epoch, if task_duration is present.", "The outcome presented for a TaskTimer is a sanitized string prefixed with 'Timer:' and includes the timer's current value plus the superclass’s outcome."]}
{"file": "app/models/organization.rb", "type": "model", "domain_rules": ["Organizations have a defined lifecycle with states: creating (initial), initializing, demodata_generating, and completed; transitions are limited to the explicit events (initialize data, generate demo data, complete).", "When an organization is created it will generate initial data by default (create_initial_data true), invoking an initialization workflow; demo data can be generated via a separate operation.", "An organization must have a name (which is unique) and a document_prefix.", "Organizations can upload a logo subject to a maximum file size; providing cropping parameters triggers recreation of logo versions.", "An organization owns many domain resources (users, equipment, procedures, projects, teams, documents, subscriptions, etc.); those associated records are removed when the organization is deleted.", "An organization may have one SSO provider (editable via nested attributes).", "Feature toggles are stored as a list and only recognized toggles may be enabled; enabling/disabling updates the toggles list and feature_toggled? reports whether a given feature is active. (Available toggles are explicitly defined; e.g., digestor.)", "Subscriptions are classified as past, future, or active; active_subscription returns the first active subscription. users_limit is derived from the active subscription (or zero if none).", "all_subscriptions_ended? returns true when there is at least one past subscription and no active subscription.", "Tag manipulation methods only operate on existing attributes: append_tag adds a tag ensuring uniqueness; remove_tag deletes a tag. Both persist the organization record.", "Scoped counts are available for reporting: equipment_count and users_count (users_count excludes users linked to equipment).", "Auditing of organization changes is enabled (paper trail), with some metadata ignored (e.g., updated_at).", "Organizations are implicitly ordered by creation time when no other order is specified."]}
{"file": "app/models/import_record.rb", "type": "model", "domain_rules": ["An import record must belong to an organization.", "An import record may be associated with an import job (optional).", "An import record may reference a domain object via a polymorphic recordable (recordable_type/recordable_id) — this link is optional.", "Each import record captures an outcome via a boolean 'successful' flag; records can be filtered as successful or failed.", "Import records are ordered by creation time (created_at) for default queries and exports.", "Import records store imported data in a 'fields' payload that is used for CSV export.", "CSV export can include the recordable_id as an ID column when requested and emits rows in ascending created_at order.", "Import records are versioned/audited (changes tracked), with routine updated_at changes ignored in the audit history."]}
{"file": "app/models/folder.rb", "type": "model", "domain_rules": ["A Folder is a hierarchical container that belongs to a single organization.", "Folders are organized in a parent/child tree structure and children are ordered by folder name.", "A Folder can contain many types of organizational resources (batches, consumables, data entries, documents, equipment, events, facilities, procedure templates, products, projects, reviews, services, suppliers, tasks, clinics).", "Resources reference their containing folder via a parent relationship (parent_id); removing a folder does not delete those resources but clears their parent link (parent_id is set to null).", "Only procedure templates (scoped as templates) are associated via the procedures relationship — non-template procedures are not included here.", "Folders support tree traversal and path resolution: a folder’s full path is the concatenation of its ancestor names joined with '/' to show its location in the hierarchy."]}
{"file": "app/models/project.rb", "type": "model", "domain_rules": ["A project must belong to an organization (cannot exist outside an organization).", "A project may optionally be placed in a folder (parent), allowing hierarchical organization.", "A project must have a name.", "A project manages many resource types: equipment, data, documents, products, consumables, procedures, batches, and comments.", "Deleting a project destroys its associated resources (equipment, data, documents, products, consumables, procedures, batches, and comments).", "When a project is deleted, project membership records are preserved but their project association is cleared (project_members are nullified).", "Users are linked to a project via project_members (project membership records represent user associations).", "Project changes are versioned/audited (paper trail) with trivial updated_at-only changes ignored.", "Project lists are ordered by creation time by default (created_at).", "Search for projects respects user authorization and performs case-insensitive partial matching on project name; an empty search returns the authorized projects limited by a default maximum (5).", "Project exports (CSV) produce rows with project name and description ordered by creation date."]}
{"file": "app/models/via_thaw_cryovial.rb", "type": "model", "domain_rules": ["This model represents a specialized thawer specifically for cryovial thawing operations (it is a Thawer subtype).", "Each ViaThawCryovial instance encapsulates a cryovial thawing operation and its lifecycle.", "A thawing operation manages multiple associated thawing results (one-to-many relationship to thaw results).", "Associated thawing results are conceptually tied to the parent thaw operation and should be removed when the operation is removed."]}
{"file": "app/models/result_akta_ready_hda.rb", "type": "model", "domain_rules": ["Records represent Akta Ready HDA chromatogram measurements (item_name, item_value, logged_at, item_hour) tied to a datum (datum_id).", "item_value must be numeric and is unique within the scope of a single datum_id, item_name and logged_at (no duplicate measurements at same timestamp for same channel).", "CSV import/export only allows the fields: item_name, logged_at, item_hour, item_value.", "Only ZIP files are accepted for import; non-ZIP files or ZIPs missing Result.xml are treated as unsupported and trigger an unsupported-file event and import_datum end_time update.", "Result.xml must contain Result.Name and Result.CreatedBy; those populate the import datum's name and operator_instrument.", "Import sets the datum's start_time and end_time based on the first and last result logged_at produced from the chromatogram data (start_time initially set then overridden by first result).", "Chromatogram parsing requires a matching chromatogram .Xml and associated binary files; CoordinateData.Amplitudes (amplitudes) and CoordinateData.Volumes (volumes) must be present to produce point data.", "Amplitude/volume pairs with blank values are skipped; unique volume–value pairs per curve generate volume records and amplitude points generate result records.", "Logged timestamp for each data point is calculated from ChromatogramStartTime adjusted by ChromatogramStartTimeUtcOffsetMinutes and the curve distances: item_hour = DistanceToStartPoint/60 + (DistanceBetweenPoints/60)*index; logged_at = start_time + item_hour.hours.", "Curve names from the instrument are normalized to canonical item_name values (e.g., 'UV_131' → 'uv_1', 'pH_121' → 'ph', many others) before storing results.", "Fraction data from EventCurves are captured as ResultAktaReadyFractionHda records and included in the same import process.", "All result, volume and fraction records created from a single import are saved in a single database transaction using bulk import; the datum's extended_header is updated with the run log.", "After import, the earliest and latest results trigger updates to the datum and equipment state (methods invoked to update result-in-datum and last-result-in-equipment)."]}
{"file": "app/models/equipment_reservation.rb", "type": "model", "domain_rules": ["A reservation must reference an equipment and an organization.", "Exactly one owner must be specified: either a user or a team (mutually exclusive).", "started_at, ended_at and reason are required fields.", "started_at must occur before ended_at.", "Reservations for the same equipment cannot overlap in time (overlap check is inclusive of endpoints).", "When user_id is changed, the reservation is validated against the user privacy policy.", "Finished reservations (ended_at <= current time) cannot be deleted; deletion is aborted and an error is added.", "A reservation can be associated with a batch schedule; creating/updating/destroying a reservation triggers recalculation and saving of the batch schedule's start/end dates.", "A reservation is considered in progress if the current UTC time is between started_at and ended_at, and finished if current UTC time is on or after ended_at."]}
{"file": "app/models/result_incubator.rb", "type": "model", "domain_rules": ["Represents an incubator measurement result recording CO₂ and temperature values at a specific timestamp (logged_at).", "Records are treated as time-ordered by logged_at (implicit ordering column).", "Each result includes setpoint and actual measurements for CO₂ and temperature: co2_set, co2_actual, temp_set, temp_actual.", "CSV export is limited to the whitelisted fields: logged_at, co2_set, co2_actual, temp_set, temp_actual — other attributes are excluded from CSV exports.", "There is no custom CSV field mapping defined for this model.", "File import is not supported for this model (import_file returns nil).", "This model relies on shared behavior/constraints provided by the DatumResults module (common result validations/logic are applied via that module)."]}
{"file": "app/models/result_xuri.rb", "type": "model", "domain_rules": ["Only .rb and .csv uploads are accepted for Xuri imports; unsupported file types mark the datum's end_time, log an error Event (unsupported_file_type), and the import fails.", "Imported files are expected to be tab-separated and only contain a predefined set of whitelisted fields (CSV_WHITELISTED_FIELDS); there is no field remapping (CSV_FIELDS_MAP is empty).", "The importer skips the first three rows of the file (presumed header/metadata) and then creates one ResultXuri per remaining row, attaching datum_id so every imported row belongs to the provided datum.", "After import the datum is updated: name is set to the uploaded filename, end_time is set to now, and start_time is computed as now minus the maximum hour_ph (in hours) among the datum's results if any exist, otherwise start_time = 24 hours ago.", "The most recent result by hour_ph is adjusted: its logged_at is set to start_time + hour_ph.hours if hour_ph is present, otherwise to start_time; that latest result sends webhooks with results_skipped: true.", "A warning Event (import_file_missing_timestamps) is created after a successful import to indicate missing timestamps; Events include equipment_id, datum_id and organization_id from the datum.", "Only the whitelisted header fields will be parsed/created on imported ResultXuri rows; unexpected fields are not mapped or accepted.", "ResultXuri records are implicitly ordered by logged_at (implicit_order_column = :logged_at).", "UNICORN_FIELDS defines the set of measurement fields considered primary for certain integrations/processing (e.g., hour_co2, hour_o2, hour_do, hour_temp, hour_ph, rock_speed/angle, media/pump weights).", "The import method returns a truthy value (whether the datum has any results) on accepted formats and nil on unsupported formats, indicating success vs failure to callers."]}
{"file": "app/models/user_reservation.rb", "type": "model", "domain_rules": ["A reservation belongs to a reserved user and an organization (batch_schedule association is optional).", "started_at, ended_at, and reason are required for every reservation.", "started_at must occur before ended_at.", "A user cannot have overlapping reservations: a new reservation must not overlap any existing reservation for the same reserved user (inclusive interval overlap).", "Reservations must fall within the reserved user's configured work schedule: allowed workdays, workday start and end times, and the user's workday timezone.", "The reserved user must have complete worktime settings (workdays count, workday_start, workday_end, and workday_timezone) before a reservation is accepted.", "The reserved user's workday timezone must be a valid timezone identifier.", "Worktime validation accounts for overnight shifts (workday_end earlier than workday_start) and cases where a work period spans midnight (start may be treated as previous day's work period when appropriate).", "The reservation weekday must be one of the user's selected workdays (bitmask-based weekday selection is respected).", "Finished reservations cannot be deleted; attempting to destroy a reservation that has already finished is prevented and appropriate errors are added (including to the batch_schedule when destroyed by association).", "When reservations are created, updated, or destroyed, an associated batch_schedule (if any) will have its start/end dates recalculated and persisted to reflect current associated reservations.", "reserved_user_id changes trigger a privacy-policy validation (user_privacy_policy) before being allowed.", "in_progress? returns true when the current UTC time is between started_at and ended_at; finished? returns true when current UTC time is on or after ended_at.", "There is a scope to query reservations that have not yet started (started_at in the future) and a scope to find reservations overlapping a given time interval."]}
{"file": "app/models/task_print_consumable_label.rb", "type": "model", "domain_rules": ["This task represents printing an approved consumable label for a specific reference within a procedure.", "A reference is required and must be unique within the same procedure (only one print task per reference per procedure).", "The task may be linked to a Label (optional), but an approval check requires that the label is approved, belongs to the same organization, and is of type 'consumable'.", "approved_label? is true only when an approved consumable label exists that matches the task's label_id and organization.", "The task's outcome reflects completion status ('complete' vs 'incomplete'), appends the parent task outcome, and is sanitized before presentation.", "The task delegates retrieval of the business object to the referenced task (associated_object) and caches that result for future use."]}
{"file": "app/models/result_ceres_box_two.rb", "type": "model", "domain_rules": ["Records represent timestamped sensor and state readings for the Ceres Box Two system and are implicitly ordered by the :time column (time-series data).", "CSV input/output is restricted to a fixed whitelist of sensor and metadata fields (CSV_WHITELISTED_FIELDS) — only those named attributes (timestamps, state IDs, many sensor measurements, pump/tilt/valve statuses, protocol info, power/microcontroller readings, masses, currents, etc.) are allowed for CSV operations.", "CSV field name mappings are empty by default (CSV_FIELDS_MAP is empty), and the model does not support file-based import through import_file (import_file returns nil).", "dk_engagement_state is constrained to one of: Disengaged, Intermediate, Engaged, or Unknown.", "dk_drawer_state is constrained to one of: Closed, Intermediate, Open, or Unknown.", "media_storage_door_state is constrained to one of: Closed, Intermediate, Open, or Unknown.", "pump_direction_1 and pump_direction_2 are constrained to Backward or Forward.", "tilt_position_1 and tilt_position_2 are constrained to one of: Home, Back, HomeSequenceStart, Drain, Mixing, or Undefined.", "The model includes shared datum/result behaviors via the DatumResults module (indicating it follows the system's common datum/result conventions and behaviors)."]}
{"file": "app/models/supplier.rb", "type": "model", "domain_rules": ["A supplier must belong to an organization.", "A supplier may be placed in an optional parent folder (folder association).", "Suppliers can have many consumables which are deleted if the supplier is deleted.", "Suppliers can have many documents which are kept but disassociated (nullified) if the supplier is deleted.", "Supplier name is mandatory.", "Website, if provided, must be a valid URL.", "Supplier status must be one of: prospective, under_review, approved, suspended, disqualified, or ceased_trading.", "When status is approved, suspended, or disqualified, last_assessed must be present.", "Only specific fields are allowed for formula access: read fields = name, status, last_assessed, next_assessment; writeable fields = name, status, last_assessed, next_assessment.", "Default implicit ordering of suppliers uses the created_at timestamp.", "CSV export outputs suppliers ordered by name (asc) and includes columns: name, website, contact, email, phone.", "Search is policy-scoped and matches suppliers by name or website (case-insensitive partial match); when search is blank it returns a limited policy-scoped list (default limit 5).", "UI representation: suppliers with status suspended or disqualified should be shown with an alert indicator; other statuses use a plain label.", "Summary presentation: show 'name: contact' when contact is present, otherwise show just the supplier name."]}
{"file": "app/models/result_sepax2.rb", "type": "model", "domain_rules": ["This model represents measurement/result records produced by the Sepax2 system (Sepax2 results).", "Result records are ordered by the time field by default (implicit ordering column = :time).", "CSV import for Sepax2 results only allows the following whitelisted fields: logged_at, time, chrono, line, ch, leak, ct_speed, volume, pos, pis_speed, red, blue, t_couv, t_couv_vaot, t_couv_vaor, t_int, t_uc, dt, five_v, red_refl, blue_refl, bag, state_id, true_pis_pos, cover, header1, header2, header3, header4, header5, header6.", "There is no default CSV field mapping (CSV_FIELDS_MAP is empty) — incoming CSV headers must match the whitelisted field names unless a mapping is provided elsewhere.", "The model includes shared behavior/validation from the DatumResults module, so additional domain rules may be enforced by that module.", "No model-level associations, validations, uniqueness constraints, or callbacks are defined in this class itself (no other domain-enforced relations or constraints are declared here)."]}
{"file": "app/models/procedure.rb", "type": "model", "domain_rules": ["Procedure represents a Standard Operating Procedure (SOP) and can be either a template (template=true) or a runnable instance (template=false).", "Templates get an organization-scoped document_prefix and auto-incremented document_number when created unless incrementing is explicitly skipped; the organization’s last_sop_document_number is updated when a new template number exceeds it.", "A procedure is considered 'finalized' when approved is true and it is not a template; templates require approval before being used as finalized SOPs.", "Approval vs verification flow: templates require approval (requires_approval?), and some procedures require verification (requires_verification) — when verification is required the record must be verified (verified=true) in addition to approval for certain transitions.", "Timestamps are set automatically: assigned_at when a user is assigned (and not a template), finalized_at when approved/finalized, and verified_at when verified.", "When a procedure is just approved (and verification is not required) or just verified (when verification is required) and it belongs to a batch, the batch’s next procedure is started automatically.", "Procedures belong to an organization and may belong to user, editor, team, project, facility, batch, folder and reviewer/approval teams; many of these associations are optional but drive workflow, batching and review behavior.", "Procedures have many ordered tasks (tasks exclude headers by default). Tasks drive execution state (current_step/current_loop_step), verification status, deviations, locked resources and exported materials.", "Resetting a procedure for a user reverses task progress and loops, clears loop-specific tasks, resets step tracking, releases locks on locked consumables and products, and saves without validation.", "Tasks can lock consumables/products; locked consumables/products can be released (and consumables optionally marked 'consumed') when a procedure is reset or completed.", "Deviations are tracked by Deviation records attached to either the procedure or its tasks; the presence of deviations affects warnings and status (deviation_ongoing / deviation_complete).", "A procedure shows a warning when deviations exist and there are no approved acknowledgements for the procedure.", "Review workflow visibility depends on a workflow existing, legacy assigned reviews, and whether verification is required and completed — review is shown only when appropriate for finalized/verified state.", "Status computation prioritizes review rejection (aborted), deviations (ongoing or complete), finalized, otherwise in_progress; status affects UI presentation (icons and style).", "Only authorized users may start a procedure for themselves; starting for a team clones the template into a non-template instance, optionally linking it to a batch and updating batch progress.", "Cloning a procedure for starting or templating creates a non-template, unapproved replica with tasks duplicated, execution fields reset, and organization-specific sop_index incremented and persisted.", "Saving progress marks tasks up to the current step as progress_saved and applies provided conditionals and formulae; saving verification progress marks verified tasks as verification_progress_saved.", "Short_name must be 3–20 characters when present, and short_name must be present for favourite templates (favourite && template).", "Name is required. User assignments trigger a privacy policy validation when user_id changes.", "Task validation is normally enforced but skipped when only current_step or current_loop_step are being updated to avoid loading all tasks during incremental updates.", "Materials HTML is sanitized of JavaScript for templates being updated and images in materials may be uploaded to S3 for storage/export.", "Procedures expose lists derived from completed tasks: product_list, consumable_list (summing locked units), equipment_list, user_list, verifiers_list, and data_list for SOP-linked data.", "A special placeholder '<TYPE>' in document_prefix is replaced with 'SOP' when setting document_prefix.", "Templates are queryable via scopes (templates, approved_templates) and pending instances are those without a user assigned."]}
{"file": "app/models/via_thaw_cryovial_result.rb", "type": "model", "domain_rules": ["Represents the outcome/result of a cryovial thaw operation within the ViaThaw workflow.", "Each ViaThawCryovialResult is associated with a specific ViaThawCryovial (i.e., the result pertains to a particular cryovial thaw).", "This model is a specialized kind of generic Result used to capture status/details of the cryovial thaw step in the workflow."]}
{"file": "app/models/product_reservation.rb", "type": "model", "domain_rules": ["A product reservation must be associated with a product and an organization (batch_schedule is optional).", "A reservation must have a start time (started_at), end time (ended_at), a reason, and a positive quantity (> 0).", "The reservation start time must be before its end time; otherwise validation fails.", "Reservations are considered in progress when the current UTC time is between started_at and ended_at, and finished when the current UTC time is on or after ended_at.", "Finished reservations (ended_at <= now) cannot be destroyed; attempts to delete them are aborted and error messages are added (including to batch_schedule associations if applicable).", "When a reservation is created, updated, or destroyed, the associated batch schedule (if any) has its start and end dates recalculated and saved based on its associated reservations.", "Reservations can be queried for time overlap using the rule: started_at <= given_end AND given_start <= ended_at (i.e., standard interval overlap).", "Reservations that start in the future are identifiable via the 'not_started' rule (started_at > now)."]}
{"file": "app/models/task_product_location.rb", "type": "model", "domain_rules": ["This task associates a product or a consumable (stored in the same 'value' field) with its storage location; product is preferred if both are present.", "The 'reference' field is required and must be unique within the same procedure (reference uniquely identifies the task per procedure).", "An optional associated_object_version_id can be provided: the model will try to return the requested version; if that version has a subsequent version it returns that next version's reified state, otherwise it returns the current association.", "If the specified version id cannot be found, the model raises Exceptions::RecordVersionNotFound for the association and version id.", "The task's outcome includes the associated object's storage_vessel name, storage_unit name, and location (joined with the superclass outcome) and is sanitized before being returned.", "There is an intended validator (QrCodeProductValidator) to ensure the 'value' refers to an existing Product, but this validation is currently commented out/disabled."]}
{"file": "app/models/result_cytometer.rb", "type": "model", "domain_rules": ["Only FCS files (.fcs) with header version FCS3.0 are accepted for import.", "The FCS file must be in list mode ($MODE == 'L'); other modes are rejected.", "Only supported byte orders and numeric datatypes are accepted; unsupported endianness or datatypes cause the file to be rejected.", "FCS header fields populate the parent datum: name (from EXPERIMENT NAME or filename), start_time ($DATE + $BTIM), end_time ($DATE + $ETIM), operator_instrument ($OP), and comment (CREATOR).", "Parameter channel names are mapped to model attributes: TIME -> logged_at; FSC-A/H/W -> fsc_area/fsc_height/fsc_width; SSC-A/H/W -> ssc_area/ssc_height/ssc_width.", "Any other parameter channels are recorded as sequential pN_area fields and their original names are saved in the datum's extended header.", "Rows are imported only when a TIME value (logged_at) is present; logged_at is computed as datum.start_time + TIME seconds.", "Only model columns that actually exist are inserted; unknown columns are ignored (and warned in logs).", "Bulk insertion is performed in batches (rows grouped) to store large numbers of cytometry measurements efficiently.", "After import, the first and last measurements (by logged_at) update related datum and equipment state (update_result_in_datum and update_last_result_in_equipment).", "A webhook is delivered for the last result after a successful FCS import to notify downstream systems (results_skipped: true).", "There is a predefined whitelist of CSV field names that represents the allowed CSV columns for cytometer results (logged_at, fsc_*, ssc_*, and p1..p20_area).", "If the file fails validation or processing at any point, the import is aborted and the parent datum is not left in a partially-imported state."]}
{"file": "app/models/webhook.rb", "type": "model", "domain_rules": ["A webhook must be associated with an organization.", "A webhook has many webhook deliveries; deliveries are removed if the webhook is deleted.", "Webhook status is one of: failed, succeeded, or registered; a failed webhook is reported as HTTP 500 and can expose parsed debug information.", "A webhook must declare a trigger (required) from a fixed set of event types (e.g. datum_created, datum_completed, result_created, cellcounter_result_created, chromatogram_result_created, cytometer_result_created, incubator_result_created, monitor_result_created, sefia_result_created, xuri_result_created, xuri_result_hda_created, sepax_result_created, ceres_box_two_result_created, task_completed, xuri_result_one_edge_created).", "A webhook must declare an HTTP method (required) chosen from PUT, POST, GET, or DELETE.", "A webhook must have a name.", "A webhook must have a URL that is a valid URL and (unless running in development) must use HTTPS.", "Webhook URLs are validated to disallow public IP addresses by default; this public-IP check can be temporarily skipped for a block of code.", "Headers must be represented as a Hash; raw header strings are parsed into 'Key: Value' pairs and invalid header formats produce a validation error.", "There is a deterministic mapping from result object types to webhook trigger types via WEBHOOKS_MAPPINGS; unknown result classes default to result_created.", "map_webhook_type has a special-case: a ResultOneEdge whose datum’s equipment reports xuri_wave? maps to xuri_result_one_edge_created instead of the default mapping.", "When debug contains timeout indicators ('TimeoutError' or 'ReadTimeout'), the webhook exposes a 'Timeout' debug message for failed deliveries."]}
{"file": "app/models/task_picture.rb", "type": "model", "domain_rules": ["TaskPicture is a specialized Task representing a picture-upload task within a procedure.", "Every TaskPicture must have a reference identifier (reference is required).", "The reference must be unique within the same procedure (no two TaskPictures in one procedure may share the same reference).", "A TaskPicture can include an uploaded image file associated with the task (the image is optional but stored on the task).", "When rendered, the task's outcome includes the picture (if present) followed by the task's original content, and the combined output is HTML-sanitized.", "Rendered images are exposed with a predictable DOM identifier and styling class (id: picture_<task id>, class: 'task-picture')."]}
{"file": "app/models/webhook_delivery.rb", "type": "model", "domain_rules": ["A webhook delivery record belongs to a specific webhook and a specific organization (association required).", "Webhook deliveries are naturally ordered by their creation time (created_at is the implicit ordering column).", "The model represents a webhook delivery and stores a JSON response body from which domain data can be read.", "Only the JSON :result field is exposed via the model's result accessor; if response_body is blank or contains invalid JSON, result is nil.", "Permitted read/formula fields are constrained to the result field and typed as string/text (PERMITTED_READ_FORMULA_FIELDS defines this whitelist)."]}
{"file": "app/models/profiles_variable.rb", "type": "model", "domain_rules": ["A ProfilesVariable represents the association between a profile and a protocol variable (links a profile to a protocols_variable).", "A profile can be of two types (SEFIA or CERES) — the mapping is associated with either a sefia_profile or a ceres_profile (they share the same profile_id context).", "Domain-specific validation rules (presence/consistency/combination constraints) are enforced by a custom ProfileVariableValidator.", "Updating a mapping updates (touches) the associated profile's timestamp, so profile modification time reflects changes to its variable mappings.", "All changes to a mapping are versioned; versions record the protocol variable's label as the item name and ignore trivial updated_at changes.", "When presented, mappings are ordered primarily by the protocol variable's data type (descending) and secondarily by the variable name (ascending), i.e., grouped by type then alphabetically."]}
{"file": "app/models/sometest.rb", "type": "model", "domain_rules": ["A Procedure has a review_workflow and zero or more Reviews collected from procedure.reviews.", "Each Review includes identity and lifecycle fields: id, status, created_at, updated_at, assigned_at (when it was assigned), and completed_at (when it was finished).", "Each Review has an approved_sig field that records an approval signature/state for that review.", "Every Review is associated with a Team (team id and name are expected/present).", "A Review may have an Assignee; when present the assignee is represented by id and name_and_email, otherwise assignee is null.", "Reviews include Notes which are serialized via to_emittable_object (notes are part of a review's emitted representation).", "A Procedure can optionally reference a first_reviewer_team and/or a second_reviewer_team (each represented by id and name).", "The assembled procedure object is intended for serialization and exposes only selected nested fields for reviews, teams, assignees, and notes."]}
{"file": "app/models/storage.rb", "type": "model", "domain_rules": ["Storage is a type of Equipment and its device_type is limited to a predefined set of storage device types from configuration.", "A storage device can be looked up by an identifier that matches either its name or its serial number.", "A storage device can contain many storage units; deleting the storage deletes its associated storage units.", "A storage device can have many accessories; only active accessories are considered in this association and they are deleted when the storage is deleted.", "Consumables can be assigned to a storage device; deleting the storage will disassociate (nullify) those consumables rather than deleting them.", "Products can be assigned to a storage device; deleting the storage will disassociate (nullify) those products rather than deleting them.", "Storage acts as the storage_vessel for consumables and products, and as the equipment owner for storage units and accessories (implying ownership relationships in lifecycle behaviors)."]}
{"file": "app/models/team_member.rb", "type": "model", "domain_rules": ["A TeamMember represents a membership linking a User to a Team and must reference both (belongs_to user and team).", "Changes to a TeamMember update the associated Team's timestamp (team is touched when the membership changes).", "TeamMember records are ordered by creation time by default (implicit order column is created_at).", "All modifications to TeamMember records are versioned/audited (has_paper_trail) to maintain history.", "Deleting a TeamMember does not automatically delete the associated User or Team (no dependent destroy configured)."]}
{"file": "app/models/result_akta_ready_fraction_hda.rb", "type": "model", "domain_rules": ["Represents AKTA HDA chromatographic fraction results associated with a datum (datum_id) and timestamp (logged_at).", "Volume must be a numeric value when present.", "The same volume value cannot be duplicated for the same datum_id, logged_at and message combination (volume is unique scoped to datum_id, logged_at, and message).", "Only the fields logged_at, volume and message are allowed for CSV export.", "There are no custom CSV field name mappings configured (CSV_FIELDS_MAP is empty).", "Includes shared datum result behavior via the DatumResults module (standard result-related logic is applied)."]}
{"file": "app/models/task_sop_datum.rb", "type": "model", "domain_rules": ["TaskSopDatum represents a Task that can be linked to an SOP datum (the link is stored in the task's 'value' field); the link is optional so a task may exist without an associated datum.", "Every TaskSopDatum must have a reference value, and that reference must be unique within the same procedure (one reference per procedure).", "When resolving the associated object, if an associated_object_version_id is provided the system will attempt to load that datum version; if that version has a subsequent ('next') version it will reify that next version (presenting the subsequent state), otherwise it falls back to the current datum.", "If a requested associated version cannot be found, the code raises a domain-level RecordVersionNotFound error (missing versions are treated as an application-level error).", "Task outcome output prepends an edit link for the associated datum (using the associated object's summary as link text) when an associated object exists, then appends the standard task outcome; the combined output is sanitized before being returned.", "The task–datum association is bidirectional (datum has task_sop_data) so SOP data are aware of their related tasks."]}
{"file": "app/models/application_record.rb", "type": "model", "domain_rules": ["All models can convert various string/blank inputs to booleans via a shared boolean_cast helper — boolean attributes should tolerate string-like values and be cast consistently.", "Enumerated attributes are expected to have localized display names under I18n keys formatted as activerecord.attributes.<model>.<enum_name_plural>.<enum_value> for human-readable enum labels.", "Versioning: when a model exposes versions, the domain version is the latest version's index converted to a float and returned as a string; for records without versions the default domain version is '0.1'.", "Change detection considers nested autosave associations: a record is treated as changed if it or any nested autosave-associated records have unsaved changes.", "Models may access the current user context via a delegated current_user (provided by the User context), implying model logic can depend on the current user.", "Models may use view helper methods via delegation to ApplicationController.helpers (note: this is a shared convenience for presentation-related logic within models)."]}
{"file": "app/models/result_xdr10.rb", "type": "model", "domain_rules": ["A ResultXdr10 record is associated with a datum (datum_id) — imported rows are created and linked to the given datum.", "Only tab-separated .rb device log files are accepted for import; files with other extensions are not processed and are logged as unsupported.", "Imported files must use a predefined whitelist of sensor/flag fields (the model’s CSV_WHITELISTED_FIELDS); only those columns are expected/handled.", "The importer skips the first row (header) and treats each subsequent row as a data record to create a ResultXdr10 entry.", "Each created result is stamped with logged_at = current time when imported.", "run_hours is the model’s implicit ordering column and is used to compute the datum’s start_time (start_time = now - max(run_hours) hours when results exist); end_time is set to now and the datum name is set to the filename.", "After import the latest non-nil logged_at result will trigger delivery of webhooks (invoked with results_skipped: true).", "Import errors are caught and logged; failures do not propagate exceptions (import returns nil on error).", "The import method returns a truthy indicator when results exist for the datum after processing, otherwise it returns nil for unsupported files or on failure."]}
{"file": "app/models/via_thaw_cryobag.rb", "type": "model", "domain_rules": ["This importer ingests cryobag thaw CSV data to create a thaw record (metadata), a sequence of measurement results, and associated events.", "The thaw record's start time is determined from the CSV header timestamp when present; otherwise the current time is used.", "A thaw record is considered a duplicate and will not be reprocessed if an existing record for the same equipment serial number and the same start time already exists (scoped by the importing user's access).", "The thaw end time is set to the timestamp of the last measurement in the CSV body, or to the start time if no measurements exist.", "The thaw name is taken from the header Name field, defaulting to 'Unknown' if absent.", "Therapy epoch and start epoch are recorded from the parsed start epoch value; fill volume is recorded as a '<n> ml' profile_fill.", "Container profile is derived from the header Overwrap flag: 'Overwrapped Cryobag' when present, otherwise 'Unwrapped Cryobag'.", "A summary comment is generated from footer warnings: 'Successful thaw' when no warnings, otherwise 'Thawed with N warning(s)'.", "The raw API payload is stored with null bytes removed.", "For each body datapoint a Result is created with a logged timestamp (datapoint time or start time) and multiple measurement channels (examples: PV3, PV1, OP0/OP1/OP2/OP3, IR0–IR4, energy, duration, estimate, P0–P3, I0–I3 etc.).", "For each footer entry an Event is created with code, message, epoch-based date, privileged flag, organization linkage, and is marked as a warning when indicated by the entry.", "The newest result triggers webhook delivery (with a short delay) after creation.", "Results, events and the thaw record are conceptually expected to be created atomically in one transaction (i.e., all-or-nothing), although the current implementation does not enforce this."]}
{"file": "app/models/consumable.rb", "type": "model", "domain_rules": ["A consumable belongs to an organization and may be associated with a project, supplier, storage vessel/unit, batch, owner (user), consumable template (master), certificate of analysis (document), folder (parent) and facility.", "Template records (template: true) are treated differently: many fields required for normal consumables (consumable_template_id, lot_number, status, expiration_date) are optional or skipped for templates.", "Every non-template consumable must reference a consumable_template_id (template/master) and must have manufacturer, model and catalog number.", "Lot number and status are required for non-template consumables; expiration_date is required on creation for non-template items.", "If create_consumable_in_sop is set, consumable_template_id is required even for non-template flows.", "Status must be one of the predefined states (quarantined, in_stock, checked_out, consumed, lost_destroyed, rejected, recalled) and unit/price quantity unit must be one of the configured quantity units.", "If a price is present, a price quantity must be provided and its quantity unit must match the item's unit; a mismatched unit is rejected.", "Price currency must be one of the supported currencies when provided.", "Custom numeric fields custom1–custom6 must be numeric when present; custom9 and custom10 are validated as dates when importing from CSV.", "Owner changes are validated by a privacy policy validator; membership in facility/project is validated when the application uses projects/facilities.", "New consumables default received_date to now and default unit to 'mL' when created without values.", "Cloning a consumable clears its certificate_of_analysis association on the copy.", "When storage_unit changes, storage tracking is updated: removing an item records accumulated storage_duration and clears stored_at; putting into storage increments storage_count and sets stored_at; moving between units adjusts item counts and free space on both old and new storage units.", "Storage unit item counts and free space are recalculated on moves using storage unit capacity and fixed_capacity settings.", "An item is considered missing if its status is in_stock but it has no storage_unit assigned.", "An item is considered expired only when its status is in_stock, checked_out or quarantined and its expiration_date is in the past.", "total_quantity is number_of_units × quantity (nil treated as zero); available_quantity sums total_quantity across all consumables of the same consumable_template_id.", "Changing lot_number (for non-templates) triggers background retrieval of a certificate of analysis.", "Assigning a certificate_of_analysis will remove that document’s previous consumable link, add the document to this consumable’s documents, and associate it as the certificate_of_analysis.", "Before creation, if the current user is an approver or operator and project/facility are missing, project and facility are auto-filled from the consumable_master or the owner’s first project/facility.", "Templates require approval: a template record requires approval when template is true and approved is false.", "Search and filtering permit searching by manufacturer, model, catalog_number, lot_number and creation date, and filtering by folder, tags and statuses; search scope can be limited to a facility, project or supplier.", "Set of permitted formula read fields include model, quantity, quantity_with_unit, number_of_units, status and storage_conditions; writable formula fields are quantity, status and storage_conditions.", "Barcode-related fields include catalog_number, lot_number, serial_number and expiration_date.", "Locking information is tracked (via ErisLockable) and consumables expose the locking user's name/email for UI and search use."]}
{"file": "app/models/thawing_profile.rb", "type": "model", "domain_rules": ["A thawing profile must have a mode, a vial_type, and a name.", "Default values: requirements = 0, vial_type = 100, mode = 'gentle', approved defaults to false if not defined.", "Vial type codes map to container descriptions: 100 => 'Cryobag (no overwrap)', 101 => 'Cryobag (overwrap)', 102 => 'L1000 Cryobag'.", "Vial type determines device type: 100 and 101 => ViaThawCryobags; 102 => L1000. Supported device types are ViaThawCryobags and L1000.", "Requirements is a bitmask of flags; if a flag is set the operator must enter that field on the device before running the profile. NOTHING_REQUIRED = 0.", "Defined requirement flags: VIAL_REQUIRED, LOAD_REQUIRED, TIME_REQUIRED, FILL_REQUIRED, BOOST_REQUIRED (bit positions documented in code).", "If fill_volume is blank the profile sets the FILL_REQUIRED flag so the device will prompt the user for fill volume (deferred entry).", "load_alarm and time_alarm support explicit 'disabled' sentinel values (LOAD_DISABLED = 100, TIME_DISABLED = 999). When set to these values the device will disable the alarm and will not prompt for it.", "Virtual attribute behavior: fill_volume_virtual toggles the FILL_REQUIRED requirement bit based on blank input; load_alarm_virtual and time_alarm_virtual map blank inputs to their respective DISABLED constants and return nil when disabled.", "pre_validation enforces defaults before validation: ensures requirements is initialized, sets FILL_REQUIRED when fill_volume blank, and sets load_alarm/time_alarm to their DISABLED constants if blank.", "Validation and compatibility are enforced by custom validators ThawingProfileViaThawCryobagValidator and ThawerDisplayCompatibleValidator (device/profile compatibility rules beyond simple presence checks).", "Device-specific numeric constraints (min, max, step) and whether fields may be deferred or disabled are defined per device in VALIDATION_CONSTANTS for FILL, LOAD, and TIME (different settings for CB1000 vs L1000).", "For CB1000 (Cryobags): fill may be deferred (prompted on device) with range 50–275 step 5; load alarm can be disabled with range -50–15 step 5; time alarm can be disabled with range 90–1200 step 10.", "For L1000: fill cannot be deferred with range 10–275 step 5; load alarm can be disabled with range -70–0 step 5; time alarm can be disabled with range 120–1200 step 30.", "Profiles produce an MD5 fingerprint based on mode, vial_type, fill_volume, load_alarm, time_alarm, requirements, and name — used to identify profile content/state."]}
{"file": "app/models/consumable_master.rb", "type": "model", "domain_rules": ["ConsumableMaster represents template consumables only (records are scoped to template: true).", "A ConsumableMaster can have many consumables; deleting a template nullifies the consumables' template link rather than deleting them.", "A ConsumableMaster can have many change notifications; deleting the template nullifies those notifications' link.", "A ConsumableMaster may belong to an approval team but this association is optional.", "Up to three barcode configurations are supported (barcode_type, barcode_type2, barcode_type3), each restricted to: Not used, Linear, QR code, or Data matrix.", "Each barcode configuration has corresponding barcode_fields (arrays) which default to empty lists.", "Searchable fields for templates include manufacturer, model, catalog number, lot number, and creation date text; searches are performed within the viewer's policy scope and results are limited when no search term is provided.", "Display name is composed of manufacturer and model; if the record is archived it is prefixed with an '[Archived]' tag (configurable via method parameters for model display).", "When serializing to a messaging payload (to_cable_object), prefer values from a provided task's consumable_master attributes, falling back to the template's attributes; additional_notes come from the task or default to an empty string.", "ConsumableMaster templates do not use or associate with a project or facility (uses_project_or_facility? always returns false)."]}
{"file": "app/models/profile.rb", "type": "model", "domain_rules": ["A profile must have a name and a type.", "Profile names must be unique within the same organization and live_version_id when creating a new profile.", "Every profile belongs to an organization and may optionally be associated with an approval team, a protocol, and a user.", "Profiles may be linked to equipment via equipment_profiles; deleting a profile removes its equipment_profiles but preserves equipment. Data and change notifications that reference a profile are nullified on profile deletion.", "Profiles are versioned and tracked (draft vs approved/live). Drafts, approval status, approved_at, version_number and live_version_id are meaningful for workflow and exports.", "Draft exports are included in package exports only when a profile has a draft and the profile is approved.", "Export filename format: sanitized profile name + version_number + id, append _DRAFT if it’s a draft, with .json extension.", "JSON export content depends on profile type: SefiaProfile/CeresProfile include protocol metadata and profile variables; FreezingProfile/ExtractorProfile include ordered segments with ramp/dwell/lid/digestor settings; ThawingProfile exports specific parameters mapped from settings (mode, vial_type, fill_volume, load_alarm, time_alarm, requirements).", "ThawingProfile scopes and logic use predefined container groupings (e.g., CB1000_CONTAINERS, L1000_CONTAINERS) to filter by container types.", "A profile is considered to require approval if it is not approved (requires_approval? returns true when not approved).", "A profile is considered synced for a given equipment when that equipment_profile has a last_synced_at timestamp later than the profile's approved_at timestamp.", "Search respects user authorization and supports filtering by device_type (via equipment.assignable_profiles), full-text search on profile name (and, in extended mode, group_name and protocol name/version), sorting, and pagination.", "When building human-readable labels or links, certain internal profile types are presented with custom names (e.g., CeresProfile shown as 'Sefia expansion system') and edit/create link behavior varies by profile type.", "MD5 checksums for a set of profiles are produced by concatenating individual profile md5 values to produce a deterministic list hash for integrity checks."]}
{"file": "app/models/consumable_reservation.rb", "type": "model", "domain_rules": ["A consumable reservation belongs to a consumable and an organization; it may optionally be associated with a batch schedule.", "Reservations must have a start time (started_at), end time (ended_at) and a reason.", "started_at must be strictly before ended_at (reservations cannot have equal or reversed start/end times).", "Quantity is required and must be a number greater than 0.", "A reservation is considered in progress when the current UTC time is between started_at and ended_at (inclusive), and finished when current UTC time is at or after ended_at.", "Finished reservations cannot be deleted; attempts to destroy a finished reservation are blocked and produce errors (the batch schedule is also given an error if the destroy was invoked through that association).", "When a reservation is created, updated, or destroyed it triggers recalculation and saving of its associated batch schedule's start/end dates (if a batch schedule exists).", "An overlapping scope is available to find reservations whose [started_at, ended_at] range overlaps a given date range (useful for conflict detection).", "A not_started scope finds reservations whose started_at is in the future relative to the current time.", "Reservations are versioned/audited (paper trail enabled) to track changes over time."]}
{"file": "app/models/equipment_log/sefia_log.rb", "type": "model", "domain_rules": ["A SefiaLog belongs to an equipment and an organization (it models events tied to a specific device and org).", "Each log entry must have an action and an event timestamp (event_at) present.", "action_kind is constrained to a fixed set of named actions (PowerOn, PowerOff, Login, LogOut, Settings Changed, Application Started, Application Ended, ReportFile Created, LogFile Created, SummaryFile Created, Other).", "by_user_role is constrained to a fixed set of roles (unknown, operator, approver, admin, service, system).", "Before validation, action_kind is set from the action value when the action matches a valid action; otherwise action_kind defaults to 'Other'.", "If an equipment is associated, it must be a Sefia S-2000 device (equipment.sefia? must be true) or a validation error is added to equipment_id.", "Search behavior: by default queries are limited to 50 results unless a noLimit flag is provided; a non-empty text search across action, detail, and by_user_id removes the limit (returns all matching records) and supports sorting and pagination when limits are not applied.", "Default listing order for most queries is event_at descending (latest first), but CSV exports are produced ordered by event_at ascending.", "Exported CSV includes event_at, action, detail, by_user_id, and by_user_role for each log entry.", "To represent validation failures in APIs, an entry can be serialized to JSON together with its full validation error messages (to_hash_with_errors).", "There is an accessor skip_validate_equipment present for callers to control/flag equipment validation behavior externally (used to bypass or alter validation flow)."]}
{"file": "app/models/equipment_log/sepax_log.rb", "type": "model", "domain_rules": ["A SepaxLog represents usage of Sepax equipment and is associated with an Equipment and an Organization.", "Every log must have a published_at timestamp.", "The associated equipment must be a Sepax system (equipment.sepax2? must be true); if not, the log is invalid with an error on equipment_id. This check can be bypassed by setting skip_validate_equipment.", "Text search matches case-insensitively against log_type, category, message, and by_user_id (partial matches). When a text search is provided, result limits are lifted.", "By default listing endpoints limit results to 50 unless a noLimit flag is provided; listings support sorting (sortField/sortDirection) and pagination (page/perPage).", "CSV export of logs includes published_at, by_user_id, message, log_type, and category and is ordered by published_at ascending.", "The JSON representation intended for clients includes validation error messages under an \"errors\" key when present."]}
{"file": "app/models/equipment_log/ceres_box_two_log.rb", "type": "model", "domain_rules": ["Each log record is associated with an equipment and an organization.", "A log must have a published_at timestamp.", "Logs are intended specifically for Ceres Box Two devices (Sefia expansion system); the equipment must report ceres_box_two? true unless validation is explicitly skipped.", "Validation of the equipment type can be bypassed by setting skip_validate_equipment.", "Logs contain searchable fields: log_type, category, message, and by_user_id; text searches match any of these fields.", "Search results are paginated and limited by default (50 items) but can be overridden via extra parameters (noLimit, perPage, page, sortField, sortDirection); when a free-text search is used, result limits are removed.", "Logs can be exported to CSV with columns: published_at, by_user_id, message, log_type, and category, ordered by published_at ascending.", "When producing JSON for the client, the log representation can include its full error messages merged into the output."]}
{"file": "app/models/tags/tag.rb", "type": "model", "domain_rules": ["A tag may belong to a user; when present the associated user must be valid and changing the user triggers a user_privacy_policy validation.", "A tag may belong to an organization; on create, organization_id is automatically set from the associated user's organization or from the taggable's organization if not provided.", "A tag can be attached to a polymorphic taggable record. Only specific models are taggable (Event, Task, Procedure, Batch, Review, Project, Comment).", "There is a Task-specific association: when taggable_type is 'Task', the tag can be accessed via the task association (uses taggable_id as the foreign key).", "A tag must have a non-empty body unless it has an attached image or document (body is required only if both image and document are blank).", "Tag body content is sanitized to remove embedded JavaScript before validation.", "A tag's type/class must be one of the predefined descendant names: Tag, Acknowledgement, Note, Deviation, or Comment.", "Emitted/serialized tag payloads include id, tag type, body, taggable_type, timestamps, and basic user info (user id and name/email).", "Tags are ordered by created_at by default and support a sorted scope; queries eager-load associated user and organization by default."]}
{"file": "app/models/tags/acknowledgement.rb", "type": "model", "domain_rules": ["Acknowledgement is a specialized Tag that represents an acknowledgement related to an Event and is used to track relationship changes.", "An Acknowledgement may be associated with an Event (belongs to Event) but the association is optional — it can exist without being attached to an Event.", "When tied to an Event, an Acknowledgement tracks relation changes on that Event and records those changes as part of its versioning/audit metadata.", "Versioning/audit history is applied for acknowledgements associated with Events (capturing object relation changes for the Event).", "Events expose their acknowledgements (inverse relationship), enabling retrieval of acknowledgements for a given Event."]}
{"file": "app/models/tags/deviation.rb", "type": "model", "domain_rules": ["A Deviation is a specialized Tag representing a deviation note that is conceptually attached to a Task.", "Deviations are associated with a Task (task is the taggable), but the association is optional — deviations can exist unattached.", "The Task association is the inverse of Task#notes (deviations are treated as task notes).", "Only deviations that are attached to a Task are versioned; version records ignore updated_at and store relation change metadata.", "Changes to the task relation for a deviation are tracked (relation change tracking is enabled for the task association).", "A deviation may include a single uploaded image; when present the image URL is included in emitted payloads.", "Deviations can be exported to CSV; each row includes the related procedure name (via task.procedure), the deviation body, the owner (name/email and role), and the creation timestamp, ordered by creation time ascending."]}
{"file": "app/models/tags/note.rb", "type": "model", "domain_rules": ["Note is a Tag subclass used to store uploaded images and documents and to track association/version changes.", "A Note can be attached to an Event or a Task via the shared taggable_id; these associations are optional.", "A Note may include an image, a document, and multiple document-as-image uploads; when present these are exposed in emitted payloads (image_url, document_url, document_identifier).", "Versioning (paper_trail) is recorded only for Notes attached to Events or Tasks; relation changes are stored in version metadata and updated_at is ignored for version diffs.", "Relation changes for event and task associations are tracked for audit purposes.", "After a Note is created, if it is attached to a Task and includes a document, the system will auto-create a Document record using the Note's document and document_identifier.", "Auto-created Document records inherit organization_id from the Note and facility_id/project_id from the associated Task's procedure.", "A Document is only created when the associated Task exists and its procedure is present and not marked as a template; otherwise no Document is created."]}
{"file": "app/models/tags/comment.rb", "type": "model", "domain_rules": ["A Comment is a tag-like entity (subclass of Tag) that supports nesting.", "A Comment can be attached to a Project (optional).", "A Comment can be attached to another Comment as its parent (optional), enabling threaded/nested comments.", "A Comment can have many child Comments; deleting a parent Comment will delete its child Comments.", "Changes to which Project a Comment is related to are tracked (relation-change tracking).", "All Comment changes are versioned/audited (PaperTrail), and relation-change information is included in audit metadata."]}
{"file": "app/models/concerns/notifiable.rb", "type": "model", "domain_rules": ["A notifiable entity has a single notification setting associated with it, and that setting is removed when the notifiable entity is removed.", "Notification settings are managed as part of the notifiable entity (can be created/updated via nested attributes).", "Notifiable entities must satisfy phone-related validation rules enforced by PhonePresentValidator.", "An object is considered SMS-capable only when phone, country_code_iso2, and country_code are all present."]}
{"file": "app/models/concerns/delayed_validation.rb", "type": "model", "domain_rules": ["Provides a save_with_delayed_validation flow that attempts to persist a record while initially skipping validations.", "The save attempt is wrapped in a database transaction to ensure atomicity.", "After the save-without-validation attempt, validations are run; if the record is invalid the transaction is rolled back and nothing is persisted.", "The method returns true when the record is valid and saved, or false when validations fail (it does not raise validation exceptions).", "Ensures either a valid record is persisted or no changes are applied—no partially saved invalid records remain."]}
{"file": "app/models/concerns/paper_trail_track_associations.rb", "type": "model", "domain_rules": ["Association tracking only runs when a non-blank association name is provided and the model exposes a virtual attribute named <association_plural>_for_papertrail.", "When preparing association data for auditing, the code always reloads the record from the database (non-cached) and fetches the current collection using the pluralized association name.", "Prepared association data is a sorted mapping of associated record IDs to their display value: use the associated record's name if available, otherwise use its ID.", "prepare_associations_for_papertrail assigns the prepared mapping to the virtual <association_plural>_for_papertrail attribute but clears change tracking so no version is created for that assignment.", "persist_associations_to_papertrail first saves any pending changes, then records an 'assign' PaperTrail event and updates the virtual <association_plural>_for_papertrail attribute so the association state is captured in a separate audit version.", "Both prepare and persist operations require the instance to be persisted (an id must exist) because they reload the record via class.find(id).", "Developers must declare which associations to track by calling track_paper_trail_association, which defines the required virtual <association>_for_papertrail attribute on the model."]}
{"file": "app/models/concerns/datum_results.rb", "type": "model", "domain_rules": ["A Result may optionally belong to a Datum, but most domain operations (updating datum/equipment, webhooks, CSV export) require a datum and will no-op if missing.", "Whenever a Result is saved it may update its Datum’s first_resultable and last_resultable pointers: set as first if the Datum has none or this result’s logged_at is earlier; set as last if the Datum has none or this result’s logged_at is later.", "Whenever a Result is saved and its Datum has associated Equipment, it may update the Equipment’s last_resultable to this result when none exists or this result’s logged_at is more recent.", "A Result’s logged_at is derived from its Datum’s start_time plus the Result’s time (interpreted as seconds) and is nil if start_time or time is absent.", "CSV export writes whitelisted (and mapped) result fields in batches and, after results, also exports the associated Datum’s events; headers use a fields map when present.", "Webhook delivery: when a Result has a Datum with an Organization, it enqueues that organization’s webhooks for the result’s mapped webhook type; delivery can be scheduled (delayed) and includes a results_skipped flag.", "Akta import/parsing rules: input may be a single entry or array; EventTime values are treated as minutes offset from a chromatogram start to compute absolute timestamps; fraction entries become result-like records (datum_id, logged_at, volume, message) and run logs become dated messages; duplicate entries are removed.", "Akta event times and start times handle timezone offsets by parsing ChromatogramStartTime and subtracting the provided UTC offset (in minutes) to derive UTC start time.", "Most helper behaviors silently return when required domain attributes are absent (e.g., datum, datum.start_time, logged_at) to avoid creating invalid associations or timestamps.", "Result partial path for views is determined under the 'results' collection by underscoring/demodulizing the result class name."]}
{"file": "app/models/concerns/versionable.rb", "type": "model", "domain_rules": ["Records can have an approver and an archiver (both are Users) but these associations are optional; changes to these user IDs must satisfy a user privacy policy.", "An item is either a live version (live_version_id blank) or a draft (live_version_id present); drafts are linked to their live record via live_version_id.", "Only one draft may exist for a given live record; draft creation will rollback if a second draft would be created.", "Creating a draft duplicates the live record and resets approval metadata (approved, approved_by, approved_at, approved_sig, ready_for_approval). The draft gets live_version_id set to the original live record and may inherit user/owner as current_user if supported.", "Draft creation also duplicates relevant associations depending on the record type (e.g., segments for FreezingProfile/ExtractorProfile; equipment_profiles and profiles_variables for Sefia/Ceres profiles; process_procedures for MfgProcess; tasks for Procedure). Profile variables are re-created without running validations because of inter-variable dependencies.", "When approving a draft, the system merges the draft into the live record: copy attributes, files/pictures (for Document/ConsumableMaster), and relevant associations, update approval metadata, reassign audit/version records, then remove the draft. This merge happens inside a transaction to ensure atomicity.", "A record cannot be sent for approval, approved, or archived if its live version is already archived; attempting to do so raises an AlreadyArchivedError.", "Sending for approval sets ready_for_approval = true (and will fail if the live version is archived).", "Approving a record sets approved = true, ready_for_approval = false, records approved_at and approved_by, stores approver signature if available, and may set the user on the record if unset.", "Archiving sets archived = true, archived_at, archived_by, clears ready_for_approval, and will also archive the draft (if present) in the same transaction.", "A record's editable version is the draft if the live record is approved and a draft exists; otherwise the record itself is editable.", "Draft lookup favors a cached drafts map when available; creating a draft is lazy unless prevented by a flag (prevent_create_new).", "Version numbering is semantic (major.minor): approved live records use major.0, unapproved live records show 0.minor, drafts render as live_major.minor, and some Documents that don't require approval use minor.0.", "Major version counts are derived from version history entries where approval occurred; minor version counts exclude preview/archived changes and certain ready_for_approval toggles.", "Pending approvals for a user include non-archived, ready_for_approval items (including ready drafts) scoped to the user's approval teams; in GMP mode, items created by the user are excluded from their own pending approvals."]}
{"file": "app/models/concerns/relation_change_tracker.rb", "type": "model", "domain_rules": ["Models must opt-in to relation change tracking by declaring which associations to track; the tracked attributes are stored per model.", "Only associations whose related class appears in the module's mapping are reported; each related class has a designated display attribute (e.g., User -> name_and_email, Team/Project/Batch/Facility/Task -> name, Event -> comment).", "A relation change is recorded only when the association's foreign key changes; the tracker returns the previous and current values for that association.", "The tracker supports associations with non-standard foreign key names by resolving the association's declared foreign_key or falling back to the '<relation>_id' convention.", "The previous value is obtained by looking up the old related record by its previous id and reading the mapped display attribute; the current value is obtained by calling the same display attribute on the current associated record.", "If there is no previous foreign-key change or if the related class/mapped display method is not defined, no change is reported for that association.", "The final result is a hash keyed by association name with each value being a two-element array: [previous_value, current_value]."]}
{"file": "app/models/concerns/opc_versionable.rb", "type": "model", "domain_rules": ["Some device types require an OPC version to be set; currently this requirement applies to devices of type 'XuriWave'.", "If a device's type is in the required list, the opc_version field must be present.", "Whether opc_version is required is determined by the device's device_type attribute.", "The module exposes user-facing hints for OPC version choices (keys: 'opc_classic' and 'opc_ua') to guide valid OPC version values."]}
{"file": "app/models/concerns/eris_lockable.rb", "type": "model", "domain_rules": ["A resource can be locked exclusively by a single applicant; acquire attempts fail if someone else holds the lock or if the DB cannot grant an immediate row lock.", "Lock ownership is recorded using polymorphic locked_by (locked_by_id and locked_by_type) when those columns exist on the model.", "Models only support lock ownership tracking if both locked_by_id and locked_by_type columns exist and SKIP_INITIALIZERS is not set to '1'.", "Acquiring a lock updates the locked_by columns to the applicant and is performed under a row-level FOR UPDATE NOWAIT transaction to avoid waiting on contention.", "If a lock is held by another applicant, acquisition raises a LockError (or higher-level helper returns nil on contention), preventing concurrent modification.", "Only the applicant that holds the lock may release it; attempts to release when locked by someone else fail (return false) or raise a LockError in strict release variants.", "Helpers exist to run work while holding a lock and to ensure the lock is released afterward (one variant swallows contention errors and returns nil, another always releases in an ensure block).", "A resource is considered locked if locked_by_id is present; checking with an applicant verifies the lock is owned by that specific applicant.", "Lock operations are defensive: DB lock timeouts are translated into LockError to signal contention to calling code."]}
{"file": "app/models/concerns/filterable.rb", "type": "model", "domain_rules": ["A model including Filterable maintains a registry of named search filters declared via search_scope.", "Only filters declared with search_scope are allowed to be used — incoming filtering parameters are whitelisted against that registry.", "Filtering is applied to an initial collection by invoking each allowed filter in turn, returning the progressively filtered result.", "Blank or empty filter values are ignored and not applied.", "Each filter parameter is applied by calling the corresponding named scope with the parameter value.", "The order in which whitelisted filter parameters are iterated/applied affects the final result (filters are applied sequentially).", "Defining a search_scope both declares the scope and registers its name for future filtering use."]}
{"file": "app/models/concerns/paper_trail_skipper.rb", "type": "model", "domain_rules": ["You can perform model operations without creating PaperTrail versions by wrapping those operations in without_versioning for the specific model(s).", "Versioning is temporarily disabled for the specified model(s) only for the duration of the provided block.", "After the block completes, versioning for the specified model(s) is restored to its previous enabled state.", "Only the explicitly listed model(s) are affected—other models continue to be versioned normally.", "Changes made to the affected models while versioning is disabled will not be recorded by PaperTrail (i.e., no audit/version entries will be created)."]}
{"file": "app/models/concerns/assets_upload.rb", "type": "model", "domain_rules": ["Inline base64 images (img tags with src starting with 'data:') in HTML must be uploaded to S3 and replaced with an internal asset URL tied to a generated UUID.", "Uploaded assets are stored under an organization-specific directory (esops_assets_dir(organization_id)) and identified by the UUID key.", "Uploaded assets are private (public: false) and served via an internal API path (esops_assets_internal_api_tasks_path(uuid)).", "The content type for an uploaded image is determined by file magic detection (Marcel::Magic) with a fallback to the data: URI mime type.", "Base64 image data that is missing, blank, '(null)', or cannot be decoded is skipped and left unchanged.", "Only images whose src points to the internal assets API path are considered for download/embedding back into HTML as data URIs.", "When downloading, files are looked up from the organization-specific S3 directory by UUID; images with missing files or empty bodies are skipped.", "Downloaded files are embedded into HTML as data URIs using the file's content_type and base64-encoded body.", "Any processing errors during upload or download are silently ignored (no-op), leaving the original img tag unchanged.", "The UUID (last path segment of the internal URL) is the canonical identifier for mapping between HTML img tags and stored S3 assets."]}
{"file": "app/models/concerns/token_authenticatable.rb", "type": "model", "domain_rules": ["Records that opt into TokenAuthenticatable must have an authentication_token; if blank it will be automatically generated before the record is saved.", "Authentication tokens must be unique across all records of the model (uniqueness is enforced by checking unscoped records).", "Tokens are produced using a Devise-friendly token generator and regenerated until an unused token is found, ensuring no collisions.", "The token-generation behavior is activated by including the concern via include_token_authenticatable, which adds a before-save hook to ensure a token exists.", "Token generation and uniqueness checks are treated as internal/private behavior of the model (not part of the public API)."]}
{"file": "app/services/auth_token_service.rb", "type": "service", "domain_rules": ["Authentication tokens expire after 15 minutes.", "Each token maps to a JSON payload that includes the associated user_id.", "Each user has one current token stored under a user-specific key.", "A token is treated as valid only if both the user's token key and the token-to-payload key exist.", "fetch_or_rotate returns the existing valid token; if none is valid it generates and stores a new token.", "Storing a token writes two entries with the same TTL: token -> payload and user -> token (reverse lookup).", "token_payload returns nil when the token payload is missing or cannot be parsed.", "Tokens are generated as cryptographically secure random hex strings.", "Storing or rotating tokens requires an associated user (user.id must be present)."]}
{"file": "app/services/application_service.rb", "type": "service", "domain_rules": ["All service actions are subject to authorization checks driven by policies.", "Whether a change reason is required is determined by the record's policy.", "If a record's policy requires a change reason, a non-blank change_reason parameter must be provided.", "If a required change reason is missing, the request is marked invalid (change_reason_invalid)."]}
{"file": "app/services/import_profile_service.rb", "type": "service", "domain_rules": ["An uploaded JSON must parse to a Hash; otherwise the import fails with 'invalid_hash_json'.", "The JSON must represent a supported profile type (SefiaProfile, CeresProfile, FreezingProfile, ExtractorProfile, ThawingProfile) and provide the fields required by that type.", "SefiaProfile and CeresProfile imports must include a protocol (name and either integer version or version string) and profile_variables.", "For protocol selection, the protocol name cannot be blank and either a numeric version or a version string must be provided; the named protocol/version must exist.", "Each profile_variable must reference an existing protocol variable by name for the chosen protocol; the protocol variable name cannot be blank.", "FreezingProfile and ExtractorProfile imports must include a segments array; each segment must supply sequence, segment_type and the expected optional control attributes (ramp_target, ramp_rate, dwell_time, dwell_time_at_end, lid_alarm_on, digestor_on, digestor_speed, reduced_digestor_intensity).", "ThawingProfile imports must include parameters containing mode, vial_type, fill_volume, load_alarm, time_alarm and requirements which map to the profile's internal setting1..setting6 fields.", "Uploaded files must pass the uploader's integrity checks; integrity failures cause the import to fail with the uploader's error message.", "Profile names and group names must be unique within the same organization; if a name or group_name is already taken it is automatically suffixed with a numeric ' (n)' and incremented until unique.", "Name suffixes use the pattern ' (number)' at the end of the name; when generating a unique name the base is taken as the string before the last such suffix and numbering starts from the suffix number or 1 if none.", "If a CeresProfile is created it uses delayed validation saving semantics; other profile types use standard save behavior and surface validation errors on failure.", "Missing or unrecognized protocol, protocol variable, JSON parse errors or unknown profile subclass produce explicit localized error messages indicating the specific failure (e.g., application_or_version_does_not_exist, protocol_variable_name_does_not_exist, invalid_json, parameter_type_unrecognized)."]}
{"file": "app/services/equipment_service.rb", "type": "service", "domain_rules": ["All equipment operations require authorization of the current user for the specific action.", "When creating equipment, organization_id is set from parameters only if the current user has admin API access; otherwise it defaults to the current user's organization.", "On equipment creation an API password is generated for the equipment.", "If equipment is a gateway, its serial number defaults to the equipment hostname.", "Tags provided as a comma-separated string are split and assigned to the equipment's tags.", "A parent relationship is established when a folder_id is provided, setting equipment.parent_id.", "Creating equipment also builds a device user with role 'device', name equal to equipment name, username equal to the equipment serial number (lowercased), password equal to the equipment API password, organization set by admin API rules, and the user is created without confirmation.", "On equipment update, any existing device user is updated to reference the equipment id and to use the equipment's serial number (lowercased) as username.", "LDAP configuration is applied only if the device type has LDAP templates and LDAP is explicitly enabled; each LDAP field is cast to the appropriate type (boolean or integer) before saving.", "Equipment updates must pass a change-reason validation in order to be saved.", "Requested firmware version input is normalized to an array when present.", "Selected version change intent (selected_version_changed) is included when preparing equipment update attributes."]}
{"file": "app/services/service_result.rb", "type": "service", "domain_rules": ["Service operations return a ServiceResult that encapsulates the outcome of the call.", "A ServiceResult contains an optional payload and a boolean success indicator.", "ServiceResult instances are considered successful by default unless explicitly marked otherwise.", "The payload may be nil when there is no data to return.", "Consumers should check the successful? flag to determine whether an operation succeeded."]}
{"file": "app/services/user_service.rb", "type": "service", "domain_rules": ["All actions on a user require authorization by the current user.", "When a non-admin creates a user and no organization is provided, the new user is assigned the creator's organization.", "User tags are supplied as a comma-separated string and stored as a tag list.", "A user's username is set to their email address in lowercase.", "On user creation a secure temporary password is generated, assigned, and the user is forced to change it on first login (force_password_change = true) with password_changed_at recorded.", "User creation is subject to subscription validations before saving.", "Workdays for a user are stored as the sum of the provided workday values.", "Password changes require supplying password data (and typically current password) and a valid change reason; password_changed_at is updated on change.", "When a user successfully changes their own password they are logged out and redirected to the root route.", "Profile updates without a password are allowed only if the change reason is valid and the user passes update validations.", "Only requests with admin API access may modify all admin-permitted attributes (including direct email changes); non-admin updates exclude email by default.", "Email changes require a confirmation flow; if an email change is initiated and the stored email differs from the new one, the user is sent to an email confirmation step.", "Invalid or missing change reasons cause updates to be rejected and add a change_reason error.", "User-facing success notices are localized according to the user's language preference."]}
{"file": "app/services/twilio_number_selector.rb", "type": "service", "domain_rules": ["Use the Twilio US number when a receiver's phone number resolves to country code 'US'.", "Use the default Twilio number for non\u2011US or unknown/blank phone numbers.", "Determine a phone number's country code via Twilio Lookup and base number selection on that result.", "Cache resolved country codes in Redis keyed by a normalized phone number to avoid repeated lookups.", "Normalize cache keys by stripping non\u2011digit characters and the '+' from the phone number.", "If the phone number is blank, treat it as non\u2011US (use the default number).", "On any error during lookup or Redis access, fall back to treating the number as non\u2011US and return the default Twilio number."]}
{"file": "app/services/biosafe_manager/unsupported_file_type_error.rb", "type": "service", "domain_rules": ["Files with unsupported types must not be processed by the system.", "Attempting to process a file with an unsupported type results in an error being raised.", "The error raised for unsupported file types must include the filename.", "Error messages for unsupported file types must be localized for the user's language."]}
{"file": "app/services/biosafe_manager/fix_numeric_convention.rb", "type": "service", "domain_rules": ["Operate on a result hash of attribute names to values and a model-like result_class that exposes attribute types.", "If a result value is nil, leave it unchanged.", "For attributes whose declared type is integer, remove all commas from the string value and convert it to an integer.", "For attributes whose declared type is float, remove all commas from the string value and convert it to a float.", "Do not alter attributes whose declared type is neither integer nor float."]}
{"file": "app/services/biosafe_manager/extract_start_time.rb", "type": "service", "domain_rules": ["If datum.procedure_ref is blank or nil, no start time is extracted (returns nil).", "A start time, if present, must be encoded at the end of the procedure_ref string.", "Supported timestamp encodings are two\u2011digit year formats: YYMMDD, YYMMDDHH, YYMMDDHHMM, and YYMMDDHHMMSS.", "Parsing is performed in the equipment's configured time zone (datum.equipment.time_zone).", "The service tries formats from most precise to least precise and stops at the first successful parse.", "Only the trailing characters of procedure_ref equal to the expected format length are considered for parsing.", "If a candidate trailing substring cannot be parsed (ArgumentError), it is skipped and parsing continues with the next format.", "If none of the supported formats match the trailing substring, the service returns nil (no start time).", "procedure_ref is treated as user-provided input and may legally end with any of the supported timestamp lengths."]}
{"file": "app/services/biosafe_manager/final_updates.rb", "type": "service", "domain_rules": ["Equipment results for a datum are processed in ascending creation order (oldest to newest).", "Finalizing a datum persists the last equipment_result as the last result in its equipment and updates the datum with that result.", "Finalizing a datum persists the first equipment_result by updating the datum with that result.", "If a last equipment_result exists, webhooks are delivered indicating results were skipped (results_skipped: true).", "Only the first and last equipment_results are explicitly persisted during the finalization process; intermediate results are treated as skipped.", "If a datum has no equipment_results, no updates or webhooks are performed."]}
{"file": "app/services/biosafe_manager/get_file_type.rb", "type": "service", "domain_rules": ["File must have a non-empty body.", "File must have a non-blank filename.", "Only files with extensions .log, .pat, .rb, or .xml are supported; any other extension is classified as unsupported.", "Extension support is checked before inspecting file content; unsupported extensions short-circuit classification.", "If any line contains more than four non-empty tab-separated fields, the file is classified as a log.", "If no line contains more than four non-empty tab-separated fields, the file is classified as a pat."]}
{"file": "app/services/biosafe_manager/file_import.rb", "type": "service", "domain_rules": ["Only PAT and LOG files are supported; other file types generate an error event and stop further processing for that datum.", "Device-specific parsers (based on the equipment's device type) must be used to parse PAT and LOG files.", "PAT files are parsed only if the datum does not already have an extended header.", "LOG files are parsed only if the datum has no equipment results yet.", "The imported file is always saved to the datum record regardless of parsing outcome.", "A datum is considered ready only when it has at least one equipment result and an extended header.", "If a datum is ready and its status is blank or IN_PROGRESS, its status is set to COMPLETED.", "When a datum transitions from not-ready to ready and has an end_time, datum-completed webhooks are delivered.", "Parsing can add warning/error events that may change the datum's status, so readiness/status must be re-evaluated after parsing.", "Unsupported-file error events are recorded and associated with the datum, its equipment, and its organization."]}
{"file": "app/services/biosafe_manager/file_init.rb", "type": "service", "domain_rules": ["A file cannot be uploaded unless it has a non-empty filename.", "A file cannot be uploaded unless it has a non-empty body.", "Uploaded file content is converted to UTF-8 before further processing.", "File type is determined from the filename and the (UTF-8) file body.", "The procedure reference for a file is resolved using a device-type specific procedure lookup.", "Each uploaded file must be associated with a datum: the system finds an existing datum or creates one for the resolved procedure reference and equipment.", "Files are stored under a data-logs directory organized by equipment serial number (data_logs_dir/<equipment_serialnum>/<filename>).", "Filenames are prefixed with an 8-character random hash (based on a timestamped seed) to ensure uniqueness.", "If a generated prefixed filename already exists in storage, a new prefix is generated and retried until unique.", "Uploaded files are stored as non-public (private) in fog storage.", "The upload operation returns the associated datum, the storage path of the file, and the determined file type."]}
{"file": "app/services/biosafe_manager/get_pat_value.rb", "type": "service", "domain_rules": ["Only the device types Sefia and Sepax2 are supported for PAT value extraction.", "Supported fields for Sefia are: procedure_ref, volume, container, software_ver, protocol, operator.", "Supported fields for Sepax2 are: procedure_ref, volume, container, operator, protocol.", "Each field for a device is identified by a fixed pair of attributes: a type (one-letter string: 'R', 'N', or 'T') and a numeric code.", "For Sefia: procedure_ref => type 'R', code 17004.", "For Sefia: volume => type 'N', code 8722.", "For Sefia: container => type 'R', code 17050.", "For Sefia: software_ver => type 'T', code 17059.", "For Sefia: protocol => type 'T', code 17019.", "For Sefia: operator => type 'R', code 17040.", "For Sepax2: procedure_ref => type 'R', code 17004.", "For Sepax2: volume => type 'N', code 17007.", "For Sepax2: container => type 'R', code 17048.", "For Sepax2: operator => type 'R', code 17040.", "For Sepax2: protocol => type 'T', code 17019.", "PAT value extraction returns the value from the results entry that matches both the configured type and the configured code for the given device and field.", "If the results are empty or there is no entry matching both type and code for the requested device/field, no PAT value is returned (nil).", "The mapping of device fields to type/code pairs is fixed and used as the authoritative lookup for PAT values."]}
{"file": "app/services/biosafe_manager/find_or_create_datum.rb", "type": "service", "domain_rules": ["An equipment and a procedure_ref are required to find or create a Datum.", "If project_id is not provided, the equipment's project_id is used.", "If facility_id is not provided, the equipment's facility_id is used.", "tags default to an empty list when not provided.", "Existing Datum is looked up (within the calling user's policy scope) by organization_id = equipment.organization_id, equipment_id = equipment.id, and procedure_ref; if found it is returned.", "If no existing Datum is found, a new Datum is created with status = IN_PROGRESS and name = procedure_ref.", "When creating, the Datum is populated with product_id, project_id, facility_id, parent_datum_id, parent_id = import_folder_id, and tags as provided.", "The service is idempotent: it will not create a duplicate Datum for the same organization, equipment, and procedure_ref visible to the user."]}
{"file": "app/services/biosafe_manager/get_events_from_pat.rb", "type": "service", "domain_rules": ["Only PAT results with type 'T' are considered for event creation.", "A PAT result is an event only if its code falls within one of the provided error or warning code ranges.", "If a result's code falls in a warning range, the resulting Event is categorized as a warning; otherwise it is categorized as an error.", "Each created Event uses the datum's organization_id as its organization identifier.", "Each Event's comment is formed by joining the result label and its value (value omitted if blank).", "Each Event stores the original result code as its code value.", "The service returns a collection of Event objects constructed from qualifying PAT results.", "Classification of codes (error vs warning) is driven entirely by the provided code ranges."]}
{"file": "app/services/biosafe_manager/check_empty_log.rb", "type": "service", "domain_rules": ["A CSV log that contains only the header (one or fewer rows) is considered empty.", "When a log is empty, the system must create a warning event indicating an empty log file.", "The warning event must be associated with the datum, its equipment, and the organization.", "The warning event should include a localized comment for 'empty_log_file' and the current timestamp as the event date.", "No warning event is created for CSV logs that contain more than one row (i.e., contain data rows).", "The service returns a positive result when it creates a warning event for an empty log and a negative result when no event is needed."]}
{"file": "app/services/biosafe_manager/save_document.rb", "type": "service", "domain_rules": ["Do not create a duplicate document for the same datum if a document with the same name already exists for that datum.", "Document uniqueness is evaluated within the documents scope of the equipment's user (authorization scope).", "Every created document must be associated with a datum_id, the equipment_id of the originating equipment, and that equipment's organization_id.", "The document name is the provided filename.", "Document notes must record that the file was pushed by the device, including the equipment name and serial number.", "The document file content must be saved from the provided file body with the given filename.", "Document persistence is mandatory (creation must succeed or be treated as a failure).", "The service requires a datum, a filename, and the file body as inputs to save a document."]}
{"file": "app/services/biosafe_manager/convert_file_body_to_utf8.rb", "type": "service", "domain_rules": ["The service must ensure a provided file body is returned as UTF-8 when possible.", "If the file begins with a UTF-16 BOM (0xFF 0xFE or 0xFE 0xFF), interpret the file as UTF-16 and convert it to UTF-8.", "If the file's encoding is ASCII-8BIT, attempt to treat it as UTF-8 and return it only if the result is valid UTF-8.", "If the file is already valid UTF-8, do not modify its contents.", "If conversion or re-interpretation to UTF-8 is not successful, return the original file body unchanged."]}
{"file": "app/services/biosafe_manager/get_procedure_ref_from_filename.rb", "type": "service", "domain_rules": ["A filename must be provided and cannot be blank; otherwise an ArgumentError 'The file is empty!' is raised.", "Primary extraction of a procedure reference is done by matching the BIOSAFE procedure reference pattern.", "The BIOSAFE procedure reference format (Sefia/Sepax) is: Application ID (3) | Application version (3) | SerialNumber (4) | YY MM DD HH MM SS (timestamp).", "An example BIOSAFE procedure reference is '2011325020200917190637', representing '201 132 5020 20.09.17 19:06:37'.", "If the BIOSAFE pattern is not present in the filename, extract the procedure reference from the file basename (filename without extension) split by underscores.", "When using the underscore-split basename fallback, prefer the second component as the procedure reference if at least two components exist; otherwise use the first component.", "If neither the BIOSAFE pattern nor any basename components yield a value, no procedure reference is returned."]}
{"file": "app/services/biosafe_manager/check_trailing_pat.rb", "type": "service", "domain_rules": ["Logs for supported equipment must include a PAT procedure ID that matches the BIOSAFE procedure reference pattern.", "For Sefia equipment the PAT ID must appear in the log as 'PAT file procedure ID: <procedure_id>'.", "For Sepax2 equipment the PAT ID must appear in the final segment after the '$B$R' delimiter.", "If the required PAT procedure ID is missing for Sefia or Sepax2 equipment, a warning event labeled 'possible_incomplete_data' must be created and associated with the datum, equipment, and organization (timestamped).", "File bodies must be converted to UTF-8 before inspecting logs for the PAT procedure ID."]}
{"file": "app/services/biosafe_manager/sepax2/pat_parser.rb", "type": "service", "domain_rules": ["PAT files are converted to UTF-8 and parsed to extract protocol details.", "From a PAT the service extracts: volume, container, operator, and protocol name.", "The full parsed PAT content is stored in mydatum.extended_header.", "profile_container is set to the extracted container value.", "profile_fill is set to \"<volume> ml\" when a volume is present; otherwise profile_fill is nil.", "comment is set to \"application: <protocol_name>\" when a protocol name is present.", "operator_instrument is set to the extracted operator value.", "Events are generated from PAT content and attached to the datum; those events can change the datum status to has_warnings or has_errors via callbacks.", "Event severity is determined by numeric code ranges: ERROR codes include 9100\u20139500, 9720, 10004\u201310005, 10100\u201310110, 10112\u201310126, 10130\u201310224, 10226, 10228\u201310230, 10300\u201310401, and 12065\u201312070.", "Event severity is determined by numeric code ranges: WARNING codes include 9001\u20139099, 9501\u20139719, 9721\u20139999, 10111, 10127\u201310129, 10402\u201310501, 12035\u201312050, 12060, 12075, 16000, and 17014.", "Events must be fetched and appended to the datum after saving its extracted fields."]}
{"file": "app/services/biosafe_manager/sepax2/get_procedure_ref.rb", "type": "service", "domain_rules": ["The service accepts only PAT and LOG file types; any other file type causes an UnsupportedFileTypeError.", "File body must be present and non-empty; otherwise raise an 'empty_file' error.", "Filename must be present and non-blank; otherwise raise an 'empty_filename' error.", "File content must be converted to UTF-8 before parsing or extraction.", "For PAT files: parse the PAT content and extract the procedure reference using the PAT parser and GetPatValue under the Sepax2/procedure_ref key.", "For LOG files: extract the procedure block as the substring after the last '$B$R' and match it against BIOSAFE_PROCEDURE_REF_PATTERN to obtain the procedure reference.", "If no procedure reference can be obtained from the PAT content or LOG procedure block, derive the procedure reference from the filename using GetProcedureRefFromFilename.", "Procedure references may be encoded in filenames using the format: ApplicationID(3) | ApplicationVersion(3) | SerialNumber(4) | YY | MM | DD | HH | MM | SS (e.g., '2011325020200917190637' -> '201 132 5020 20.09.17 19:06:37').", "The service returns the resolved procedure reference (from file content when available, otherwise from the filename)."]}
{"file": "app/services/biosafe_manager/sepax2/log_parser.rb", "type": "service", "domain_rules": ["A Sepax2 log must contain a header line matching 'SN... GMAP build...' or the file is treated as corrupted and an error event is created.", "Data rows start three lines after the header marker and continue until the next blank line.", "Each data row must include a time value; rows without time are ignored.", "The datum start_time is parsed from the header's time and date elements in the equipment's time zone; if either is missing, start_time is nil.", "The datum end_time is set to start_time plus the duration parsed from the last CSV row's first field, but only if start_time exists and duration > 0.", "If the header contains a software version, the equipment's software_version is updated.", "Multiple occurrences (>=2) of the abrupt interruption marker '$T$17001' in the file create a warning event linked to the datum and equipment.", "Empty logs (no valid data rows) are detected and cause early termination of import processing.", "Only attributes matching the ResultSepax2 model are imported; values are normalized for numeric conventions before import.", "Each imported result has datum_id set to the datum and logged_at calculated as start_time plus the row's time offset when start_time is present.", "Trailing PAT checks and final datum updates are performed as part of the import workflow."]}
{"file": "app/services/biosafe_manager/sefia/pat_parser.rb", "type": "service", "domain_rules": ["PAT files contain Parameter records (LineType 'N') with ID->code, Label, and Value; other blocks (Traceability, Header, ErrorWarning) lack LineType and are mapped to types R, T, T respectively.", "The parser extracts Sefia metadata keys: volume, container, software_ver, operator, and protocol from the PAT content.", "Parsed PAT results are stored as the datum's extended_header and used to compute the datum's start_time.", "If volume is present, profile_fill is set to \"<volume> ml\"; if volume is absent, profile_fill is nil.", "profile_container is set to the parsed container value.", "If protocol is present, comment is set to \"<application translation>: <protocol>\"; otherwise comment is nil.", "operator_instrument is set to the parsed operator value.", "If software_version (software_ver) is present, the equipment record's software_version is updated; otherwise it is left unchanged.", "Events are extracted from PAT results after the datum is saved and appended to the datum's events collection.", "Event codes within the configured ERROR_CODES ranges are classified as errors; codes within WARNING_CODES ranges are classified as warnings.", "Attaching events can change the datum's status to reflect presence of warnings or errors (has_warnings/has_errors)."]}
{"file": "app/services/biosafe_manager/sefia/get_procedure_ref.rb", "type": "service", "domain_rules": ["A non-empty file body and a non-blank filename are required; missing file body or filename raises an ArgumentError.", "Only :pat and :log file types are supported; other file types raise an UnsupportedFileTypeError.", "For PAT files, attempt to extract the Sefia procedure_ref from the parsed PAT content first.", "If a PAT file does not contain a Sefia procedure_ref, fall back to extracting the procedure reference from the filename.", "For log files, search the content after the dashed separator block and apply the BIOSAFE_PROCEDURE_REF_PATTERN to find the procedure reference.", "If the log block does not contain a match for BIOSAFE_PROCEDURE_REF_PATTERN, fall back to extracting the procedure reference from the filename.", "The filename encodes procedure reference information (format: ApplicationID(3) | AppVersion(3) | SerialNumber(4) | YYMMDDHHMMSS) and is used as the fallback source for procedure references."]}
{"file": "app/services/biosafe_manager/sefia/log_parser.rb", "type": "service", "domain_rules": ["Convert incoming log file body to UTF-8 before any parsing or validation.", "Only process log lines up to the separator line containing '---------------------'; lines after that are ignored.", "Determine mydatum.start_time via ExtractStartTime before processing row times.", "Extract duration from the last row's first tab-separated field as a float; default duration is 0.", "If start_time is present and duration > 0, set mydatum.end_time = start_time + duration (seconds) and save mydatum.", "If CheckEmptyLog indicates the log is empty, abort further processing.", "Skip any CSV row that has no time value.", "Interpret each row's time value as an offset in seconds and compute logged_at = start_time + time_seconds when start_time is present.", "Treat the strings 'TRUE' for hcc_pelt_ctrl and hcm_pelt_ctrl as boolean true; otherwise they are false.", "Assign datum_id on each result to the current mydatum.id.", "Only persist attributes that exist on the ResultSefia model; drop any other columns.", "Normalize numeric formats according to the ResultSefia numeric conventions before import.", "Group rows with the same set of attributes and perform bulk import into ResultSefia for each group.", "Map common header shorthands to specific attributes (w1\u2192weight1, w2\u2192weight2, w3\u2192weight3, w4\u2192weight4, v3\u2192volt3, v5\u2192volt5, v12\u2192volt12, v24\u2192volt24, v48\u2192volt48, cu_vv1\u2192cuvv1 \u2026 cu_vv6\u2192cuvv6); otherwise derive attribute name from the header's first token.", "After importing results, run final updates for the mydatum (FinalUpdates)."]}
{"file": "app/services/chromatogram_manager/chart_chromatogram_data.rb", "type": "service", "domain_rules": ["Only curves explicitly marked as required in params (params['required_<curve_key>'] == 'true') are fetched for charting.", "For each required curve, only 'volume' and 'amplitude' fields are returned.", "Results are paginated using an integer offset and a fixed page size of 10,000 records per request.", "When a curve is not required, its data entry is an empty object.", "A mapping from curve numbers to curve keys/names (curve_info) drives which curves are processed and how they are named in the response.", "When offset is zero (first page), the service returns an additional 'max_<curve_key>' value giving the total available record count for that curve (computed ignoring offset and limit).", "Offset values are coerced to integers before use.", "Data returned for a curve is filtered by its curve_number."]}
{"file": "app/services/opc_manager/prepare_unicorn_data.rb", "type": "service", "domain_rules": ["Only include results that have either a logged_at timestamp or a non-null value in the specified hour field.", "If a result's logged_at is missing, compute it by adding the number of hours in the hour field to the result's datum.start_time.", "If datum.start_time is missing when computing logged_at, use the current time as the start point.", "The hour field represents elapsed hours to be added to start_time to derive logged_at.", "Returned records must contain the hour field, the specified value field, logged_at, and datum_id.", "Apply optional pagination: limit and offset are applied to the selected records.", "Records are ordered by the hour field ascending during selection and the final output is sorted chronologically by logged_at.", "Exclude records that lack both logged_at and the specified hour field."]}
{"file": "app/services/opc_manager/check_unicorn_data.rb", "type": "service", "domain_rules": ["Unicorn data is defined as any non-nil value present in a specified hourly field.", "The service checks a provided collection of results for non-nil values in a list of hour fields.", "If any record has a non-nil value in any of the specified hour fields, the check returns true (unicorn data exists).", "If none of the specified hour fields contain non-nil values across all results, the check returns false (no unicorn data).", "The check short-circuits on first detection: it stops and returns true as soon as any specified hour field is found to contain non-nil data.", "The service requires two inputs to operate: a results collection and a list of hour_fields to inspect."]}
{"file": "app/services/opc_manager/delete_real_time_results.rb", "type": "service", "domain_rules": ["Real-time results for a datum are deleted only when the number of saved results equals the expected results count.", "Deletion is scheduled asynchronously via a background job (DeleteXuriRealTimeDataJob).", "No deletion is scheduled if the datum is missing.", "No deletion is scheduled if the saved results count is missing.", "No deletion is scheduled if the expected results value is blank or missing.", "The decision to delete uses the saved results count snapshot taken at service initialization (datum.results_xuri_hda.count)."]}
{"file": "app/services/opc_manager/add_time_zone.rb", "type": "service", "domain_rules": ["If the provided date is nil, return nil (no timestamp is saved).", "The service must produce a correct UTC timestamp for any input date, whether the input includes a timezone offset or not.", "If the input date includes a timezone offset, that offset must be respected when determining the UTC timestamp (do not override it with the equipment's timezone).", "If the input date has no timezone offset, interpret the naive date using the equipment's configured time zone to determine the correct UTC timestamp.", "Use the equipment's configured time_zone to interpret or normalize dates when needed so stored UTC reflects the instrument's local time.", "Do not parse dates in a way that forces +00:00 (e.g., avoid DateTime.parse on the raw input), because that can produce incorrect conversions to the equipment time zone."]}
{"file": "app/services/opc_manager/find_or_create_datum.rb", "type": "service", "domain_rules": ["A procedure_ref is required to find or create a datum; if procedure_ref is blank the operation returns nil.", "If a datum with the given procedure_ref exists within the equipment user's accessible data, return that datum and do not create a new one.", "A new datum will only be created if no existing datum is found and a start_time is provided; if start_time is blank the operation returns nil.", "When creating a datum, its name defaults to the provided name or falls back to the procedure_ref if name is not provided.", "Created data items are marked with status = IN_PROGRESS.", "Created datum must be associated with the equipment and inherit the equipment's organization_id, project_id, and facility_id.", "operator_instrument and comment are optional attributes that may be set on creation.", "After a datum is created, an alert operation is triggered for that datum.", "Search for existing data is limited to the data scope available to the equipment's user (user-specific data visibility applies).", "The service behavior is idempotent for a given equipment and procedure_ref: repeated calls return the same existing datum rather than creating duplicates."]}
{"file": "app/services/opc_manager/datum_review_attributes.rb", "type": "service", "domain_rules": ["Datum start_time and end_time, if present, must be converted to the equipment's timezone before being accepted.", "Extended header log dates must be converted to the equipment's timezone according to the device type.", "XuriWave devices expect extended_header logs as part of the datum and require each log's date to be timezone-adjusted.", "AktaReady, AktaReady450, and AktaReadyflux devices expect an extended_header containing run_log and fractions arrays; each log entry in those arrays must have its date timezone-adjusted.", "If an AktaReady-family device does not provide an extended_header, the system treats it as an empty extended_header (no logs).", "Only the supported device types (XuriWave, AktaReady, AktaReady450, AktaReadyflux) have defined extended_header handling; other device types are not supported and will trigger an error.", "Only non-empty timezone-adjusted start_time or end_time values are merged into the permitted datum attributes.", "The service processes and returns the permitted attributes for a single datum after applying timezone and extended_header adjustments."]}
{"file": "app/services/opc_manager/readyflux/chart_flowrate_data.rb", "type": "service", "domain_rules": ["Data is returned in paginated slices of at most 5000 records per metric.", "Available metrics are: tran_flow, feed_flow, fe141, p203_flow, perm_vt, and flux.", "Each metric is included in the response only if its corresponding request flag is 'true' (tranFlowRequired, feedFlowRequired, fe141Required, p203FlowRequired, permVtRequired, fluxRequired).", "Pagination is controlled by an integer offset; the offset determines which slice of data is returned.", "The service returns, for each requested metric, a list of timestamped values (timestamp = item_hour, value = item_value).", "Total available counts (max_*) for a metric are calculated only when the offset is zero (i.e., for the first page).", "If a metric is not requested, it is returned as an empty list in the response.", "The response includes both the data slice for each requested metric and a corresponding max count value when available.", "Slice boundaries (limit and offset) are applied per metric so each metric's slice is independently paginated."]}
{"file": "app/services/opc_manager/readyflux/chart_permeate_data.rb", "type": "service", "domain_rules": ["Permeate measurement data is available for analytes: perm_vt, ce102, ph, and uv.", "Clients must request an analyte explicitly using a parameter flag equal to the string 'true' (e.g., permVtRequired == 'true') to receive its data.", "Results are filtered by analyte name (item_name equals the analyte key) and only include the measurement timestamp/hour and the measurement value.", "Each page returns at most 5,000 measurement records per analyte (READYFLUX_DATA_SLICE_SIZE = 5000).", "Pagination is controlled by an integer offset; offsets are applied to the returned slices.", "The total available count for an analyte (max_..._results) is computed and returned only when the request is for the first page (offset == 0).", "If an analyte is not requested, the service returns an empty result set for that analyte.", "Measurement values are aliased in the response to the analyte name (item_value AS <analyte>)."]}
{"file": "app/services/opc_manager/readyflux/chart_pressure_data.rb", "type": "service", "domain_rules": ["Service provides post-run measurement data for pressures (pt111, pt112, pt113), temperature (tmp), and permeability (perm_vt) for charting.", "Each measurement is retrieved only if its corresponding request parameter equals the string 'true' (pt111Required, pt112Required, pt113Required, tmpRequired, permVtRequired).", "Queries return item_hour and item_value (aliased to the measurement name) as the data fields used for charts.", "Results are paginated using an integer offset and a fixed slice size of 5000 records per request (READYFLUX_DATA_SLICE_SIZE).", "When the offset is zero, the service calculates the total available record count for each requested measurement (max_* values) by counting the full result set without limit/offset.", "If a measurement is not requested, the service returns an empty result set for that measurement and does not provide a total count for it."]}
{"file": "app/services/opc_manager/readyflux/chart_tank_recirculation_data.rb", "type": "service", "domain_rules": ["The service provides tank recirculation metrics for charting: feed_bag_weight, te161, and ce101.", "Each metric is fetched only if its corresponding request parameter equals the string 'true' (feedBagWeightRequired, te161Required, ce101Required).", "Data is paginated with a fixed page size of 5,000 records per request.", "An integer offset is applied to paginate results; offset is interpreted as zero-based and converted to an integer.", "When offset is zero (first page), the service also calculates the total available count (max_*) for that metric; totals are not recalculated for subsequent pages.", "Metrics are selected from the source dataset by item_name matching the metric key ('feed_bag_weight', 'te161', 'ce101').", "Returned rows include the timestamp/hour (item_hour) and the metric value (item_value, aliased to the metric name).", "If a metric is not requested, the service returns an empty result list for that metric and no total count.", "The response for each metric includes both the result set (paginated slice) and a max count field (present only when computed)."]}
{"file": "app/services/opc_manager/xuri/chart_temp_ph_data.rb", "type": "service", "domain_rules": ["Data source is selected by datum type: post-run, unicorn, or real-time.", "Four measurable series are supported: temperature (temp), temperature setpoint (temp_sp), pH (ph), and pH setpoint (ph_sp).", "Each series is included only if its corresponding request flag is set (tempRequired, tempSpRequired, phRequired, phSpRequired set to 'true').", "Results are paginated with a page size limit of 10,000 records; an offset parameter selects the page.", "When offset is zero (first page), total counts (max results) for each requested series are computed and returned.", "Post-run data is stored as item_name/item_value records and returned with logged_at timestamps and the item value mapped to the appropriate series field.", "Real-time data excludes records with nil logged_at and excludes rows where the requested series value is nil; results include logged_at, value, and datum_id.", "Unicorn data is derived from hourly-aggregated sources (hour_temp, hour_temp_sp, hour_ph, hour_ph_sp) and must be prepared/transformed before pagination.", "If a requested series has no data, an empty array is returned for that series.", "All returned records include a logged_at timestamp alongside the series value."]}
{"file": "app/services/opc_manager/xuri/chart_gases_data.rb", "type": "service", "domain_rules": ["There are three data sources: post-run, unicorn, or real-time; post-run is used if datum.xuri_post_run_data?, else unicorn if datum.xuri_unicorn_data?, otherwise real-time.", "Only metrics explicitly requested via params flags (co2Required, co2SpRequired, o2Required, o2SpRequired, doRequired, doSpRequired equal to 'true') are fetched.", "All metric queries are paginated and sliced with a maximum page size of 10,000 records (XURI_DATA_SLICE_SIZE).", "Offset is treated as an integer and used to paginate results; when offset is zero the service also computes the total available count for that metric.", "Post-run data is selected by item_name (e.g. 'co2', 'co2_sp', 'o2', 'o2_sp', 'do', 'do_sp') and uses item_value aliased to the metric name with its logged_at timestamp.", "Unicorn data is prepared via OpcManager::PrepareUnicornData using hourly keys (e.g. 'hour_co2', 'hour_co2_sp', 'hour_o2', etc.) and supports slicing and total-size computation.", "Real-time queries exclude records with nil logged_at and exclude rows where the requested metric column is nil (e.g. where.not(co2: nil)), returning logged_at, the metric, and datum_id.", "For every requested metric the response includes both a results slice and a corresponding max_* count indicating total available records (max_* only set when computed).", "Setpoints are treated as separate metrics (co2_sp, o2_sp, do_sp) and are fetched and paginated independently from their measurements.", "If a metric is not requested, its result in the returned hash is an empty array (or omitted) and no count is computed.", "Total counts for post-run and real-time are computed by removing pagination (except for unicorn where PrepareUnicornData without slice is used to get total size)."]}
{"file": "app/services/opc_manager/xuri/chart_rocking_data.rb", "type": "service", "domain_rules": ["Service provides rock speed and rock angle setpoint data for charting from three datum types: post-run, unicorn, or real-time.", "Data is paginated with a fixed page size of 10,000 records per request.", "Offset is treated as an integer; when offset == 0 the service also computes total counts for the requested datasets.", "The request parameters rockSpeedRequired and rockAngleSpRequired must be the string 'true' to include rock speed or rock angle setpoint datasets respectively.", "Post-run data is selected by item_name ('rock_speed' or 'rock_angle_sp') and uses item_value as the metric value.", "Real-time data excludes records with nil logged_at and only includes rows where rock_speed or rock_angle_sp are not null; real-time selections include datum_id and logged_at.", "Unicorn data is produced by preparing hourly datasets ('hour_rock_speed' -> rock_speed, 'hour_rock_angle' -> rock_angle_sp) via the PrepareUnicornData routine.", "All datasets are limited by the page size and offset for pagination; total (max_*) counts are computed without limit/offset only when offset is zero.", "If a dataset is not requested (parameter not 'true'), the service returns an empty array for that dataset."]}
{"file": "app/services/opc_manager/xuri/chart_mass_data.rb", "type": "service", "domain_rules": ["The service provides mass measurement data for charting from three sources: post-run, real-time, and unicorn.", "Data is returned in paginated slices with a fixed slice size of 10,000 records per request.", "Which measurements are returned depends on request flags: mediaWeightSpRequired, mediaInstWeightSpRequired, and weightRequired must be the string 'true' to include that measurement.", "For post-run data, measurements are stored and queried by item_name values: 'media_weight_sp', 'media_inst_weight_sp', and 'weight'.", "For real-time data, only records with a non-null logged_at are considered and measurement fields must be non-null to be returned.", "Unicorn data is derived from hourly aggregated sources: 'hour_media_weight_sp' \u2192 media_weight_sp, 'hour_media_inst_weight_sp' \u2192 media_inst_weight_sp, and 'hour_weight' \u2192 weight.", "Pagination uses limit and offset; when offset is zero the service also computes total counts (max_*) for each requested measurement.", "Total counts (max_media_..., max_weight_...) are computed only for the initial slice (offset == 0).", "If a measurement flag is not requested, the corresponding result list is returned as an empty array.", "Real-time result rows include datum identifiers (datum_id) alongside logged_at and the measurement value.", "Post-run result rows include logged_at and item_value aliased to the corresponding measurement name.", "Unicorn data uses a preparation step (PrepareUnicornData) to produce both sliced results and, when needed, full-size totals."]}
{"file": "app/services/akta_flux_manager/chart_pressure_data.rb", "type": "service", "domain_rules": ["Only the pressure streams explicitly requested via params are fetched: feed, permeate, retentate, and temporary pressure.", "A stream is considered requested when its params flag equals the string 'true' (feed_pressureRequired, permeate_pressureRequired, retentate_pressureRequired, pressure_tmpRequired).", "Stream data is identified by item_name values: 'feed_pressure', 'permeate_pressure', 'retentate_pressure', and 'pressure_tmp'.", "Each returned data point contains a timestamp (logged_at) and a pressure value (item_value aliased to the stream name).", "Queries return at most 10,000 records per call (AKTA_FLUX_DATA_SLICE_SIZE = 10000).", "Pagination is performed using an integer offset and the fixed slice size limit.", "Total counts for a stream (max_*_results) are computed only when offset is zero (i.e., on the first page).", "If a stream is not requested, its result set is returned as an empty list.", "The service returns both the paginated data slice and, when available, the total count for each requested stream to support charting."]}
{"file": "app/services/akta_flux_manager/chart_flow_rate_data.rb", "type": "service", "domain_rules": ["The service returns post-run flow rate and flux measurements (data collected immediately after a run).", "Available metrics: permeate_flow_rate, feed_pump_flow_rate, permeate_pump_flow_rate, transfer_pump_flow_rate, and permeate_flux.", "Each returned metric is a time series consisting of logged_at timestamp and the metric value (item_value) aliased to the metric name.", "Clients must request each metric explicitly by setting the corresponding parameter flag to the string 'true' (e.g., permeate_flow_rateRequired = 'true').", "Results are paginated: an integer offset is used and each metric query is limited to a slice size of 10,000 records per call.", "The total count for a metric (max_..._results) is computed only when the offset is zero (i.e., on the first page).", "If a metric is not requested, the service returns an empty list for that metric.", "Queries only include entries matching the specific item_name for the requested metric; other item types are excluded."]}
{"file": "app/services/akta_flux_manager/chart_mass_temp_data.rb", "type": "service", "domain_rules": ["The service prepares time-series chart data for tank level, temperature, and mixer speed.", "Each metric (tank_level, temperature, mixer_speed) is included only if its corresponding parameter flag (tank_levelRequired, temperatureRequired, mixer_speedRequired) is the string 'true'.", "Data is paginated using an integer offset and a fixed slice size of 10,000 records per metric per request.", "Each metric query returns timestamped measurements (logged_at) and the measurement value aliased to the metric name (item_value AS <metric>).", "Total counts for a metric (max_<metric>_results) are computed only when the offset is zero (i.e., on the first page).", "If a metric is not requested, the service returns an empty list for that metric's results.", "The offset parameter is normalized to an integer before use.", "Each metric is retrieved by filtering records where item_name equals the metric key.", "The 10,000 record limit is applied independently to each metric query.", "The returned payload includes both the paginated results and, when available, the corresponding total counts for each requested metric."]}
